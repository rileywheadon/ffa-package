\nonstopmode{}
\documentclass[a4paper]{book}
\usepackage[times,hyper]{Rd}
\usepackage{makeidx}
\makeatletter\@ifl@t@r\fmtversion{2018/04/01}{}{\usepackage[utf8]{inputenc}}\makeatother
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `ffaframework'}}
\par\bigskip{\large \today}
\end{center}
\ifthenelse{\boolean{Rd@use@hyper}}{\hypersetup{pdftitle = {ffaframework: Flood Frequency Analysis Framework}}}{}
\ifthenelse{\boolean{Rd@use@hyper}}{\hypersetup{pdfauthor = {Riley Wheadon; Cuauhtémoc Vidrio-Sahagún; Alain Pietroniro; Jianxun He}}}{}
\begin{description}
\raggedright{}
\item[Title]\AsIs{Flood Frequency Analysis Framework}
\item[Description]\AsIs{Tools to support systematic and reproducible workflows for both stationary and nonstationary flood frequency analysis, with applications extending to other hydroclimate extremes such as precipitation frequency analysis. This package implements the FFA framework proposed by Vidrio-Sahagún et al. (2024) <}\Rhref{https://doi.org/10.1016/j.envsoft.2024.105940}{doi:10.1016/j.envsoft.2024.105940}\AsIs{>, originally developed in MATLAB, now adapted for the R environment. }
\item[Version]\AsIs{0.1.0}
\item[Maintainer]\AsIs{Riley Wheadon }\email{rileywheadon@gmail.com}\AsIs{}
\item[URL]\AsIs{}\url{https://rileywheadon.github.io/ffa-docs/}\AsIs{}
\item[BugReports]\AsIs{}\url{https://github.com/rileywheadon/ffa-package/issues}\AsIs{}
\item[License]\AsIs{GPL (>= 3)}
\item[Encoding]\AsIs{UTF-8}
\item[LazyData]\AsIs{true}
\item[Depends]\AsIs{R (>= 4.4.0)}
\item[Imports]\AsIs{ggplot2, patchwork, httr, stats}
\item[Suggests]\AsIs{knitr, testthat, vdiffr}
\item[VignetteBuilder]\AsIs{knitr}
\item[Roxygen]\AsIs{list(markdown = TRUE)}
\item[RoxygenNote]\AsIs{7.3.2}
\end{description}
\Rdcontents{Contents}
\HeaderA{ffaframework-package}{Flood Frequency Analysis Framework}{ffaframework.Rdash.package}
\aliasA{ffaframework}{ffaframework-package}{ffaframework}
\keyword{package}{ffaframework-package}
%
\begin{Description}
This package provides tools for stationary (S-FFA) and nonstationary (NS-FFA)
flood frequency analysis of annual maximum series data. Methods are organized
by prefix to support flexible workflows:
\begin{enumerate}

\item{} \AsIs{\texttt{eda\_*}}: Explore annual maximum series data for evidence of nonstationarity to
inform approach selection (S-FFA or NS-FFA):
\begin{itemize}

\item{} Detect statistically significant change points.
\item{} Detect statistically significant temporal trends in the \emph{mean} and \emph{variability}.

\end{itemize}

\item{} \AsIs{\texttt{select\_*}}: Select a suitable probability distribution using the L-moments.
\item{} \AsIs{\texttt{fit\_*}}: Fit model parameters using the \AsIs{\texttt{fit\_*}} methods.
\item{} \AsIs{\texttt{uncertainty\_*}}: Compute return level estimates and confidence intervals.
\item{} Evaluate model performance using \code{\LinkA{model\_assessment()}{model.Rul.assessment}}.

\end{enumerate}


Alternatively, users may use the \AsIs{\texttt{framework\_*}} functions to orchestrate EDA,
FFA, or both. The package also provides additional utility functions:
\begin{itemize}

\item{} \AsIs{\texttt{data\_*}} methods load, transform, and decompose annual maximum series data.
\item{} \AsIs{\texttt{plot\_*}} methods produce diagnostic and summary plots.
\item{} \AsIs{\texttt{quantile\_*}}, \AsIs{\texttt{loglik\_*}}, and \AsIs{\texttt{lmom\_*}} implement distribution-specific computations.

\end{itemize}


Finally

This package assumes familiarity with statistical techniques used in FFA, including
L-moments, maximum likelihood estimation, and parametric bootstrap.
For an explanation of these methods, see the
\Rhref{https://rileywheadon.github.io/ffa-docs/}{FFA Framework wiki}.
For examples, see the vignettes on exploratory data analysis and flood frequency
analysis.
\end{Description}
%
\begin{Author}
\strong{Maintainer}: Riley Wheadon \email{rileywheadon@gmail.com}

Authors:
\begin{itemize}

\item{} Cuauhtémoc Vidrio-Sahagún \email{ct.vidrio-sahagun@usask.ca}
\item{} Alain Pietroniro \email{alain.pietroniro@ucalgary.ca}
\item{} Jianxun He \email{jianhe@ucalgary.ca}

\end{itemize}


\end{Author}
%
\begin{SeeAlso}
Useful links:
\begin{itemize}

\item{} \url{https://rileywheadon.github.io/ffa-docs/}
\item{} Report bugs at \url{https://github.com/rileywheadon/ffa-package/issues}

\end{itemize}


\end{SeeAlso}
\HeaderA{CAN\_05BA001}{CAN-05BA001}{CAN.Rul.05BA001}
\keyword{datasets}{CAN\_05BA001}
%
\begin{Description}
A dataframe of annual maximum series observations for
station 05BA001, BOW RIVER AT LAKE LOUISE in Alberta, Canada.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
CAN_05BA001
\end{verbatim}
\end{Usage}
%
\begin{Format}
A dateframe with 62 rows and 2 columns spanning the period 1913-2023.
\end{Format}
%
\begin{Details}
Variables:
\begin{itemize}

\item{} \code{max}: Numeric; the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: Integer; the corresponding year.

\end{itemize}

\end{Details}
%
\begin{Section}{Additional Information}

This is an \emph{unregulated} station that is not in the RHBN network. Other notable features include:
\begin{itemize}

\item{} There are no observations from 1919-1965. Missing data should be handled carefully.
\item{} The MKS/Pettitt tests do not find evidence of change points at the 0.05 significance level.
\item{} Trend detection finds evidence of a trend in variability.
\item{} If nonstationarity is assumed, RFPL uncertainty quantification fails on this dataset.

\end{itemize}


This dataset is used as a test case for failure modes in RFPL and variability estimation.
\end{Section}
%
\begin{Source}
Meteorological Service of Canada (MSC) GeoMet Platform
\end{Source}
\HeaderA{CAN\_05BB001}{CAN-05BB001}{CAN.Rul.05BB001}
\keyword{datasets}{CAN\_05BB001}
%
\begin{Description}
A dataframe of annual maximum series observations for
station 05BB001, BOW RIVER AT BANFF in Alberta, Canada.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
CAN_05BB001
\end{verbatim}
\end{Usage}
%
\begin{Format}
A dateframe with 110 rows and 2 columns spanning the period 1909-2018.
\end{Format}
%
\begin{Details}
Variables:
\begin{itemize}

\item{} \code{max}: Numeric; the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: Integer; the corresponding year.

\end{itemize}

\end{Details}
%
\begin{Section}{Additional Information}

This is an \emph{unregulated} station in the RHBN network. Whitfield \& Pomeroy (2016) found that
floods may be caused by rain, snow, or a combination of both. Therefore, practitioners should
be careful when interpreting the results of FFA on this station. Minimal human intervention in
the basin means there is little justification for change points. Trend detection finds
evidence of a decreasing trend in the mean.

This dataset is used as a test case for comparison with the MATLAB implementation of the
FFA framework. It is also an excellent introductory example to nonstationary FFA.
\end{Section}
%
\begin{Source}
Meteorological Service of Canada (MSC) GeoMet Platform
\end{Source}
%
\begin{References}
Whitfield P. H., and Pomeroy J. W. (2016) Changes to flood peaks of a mountain river:
implications for analysis of the 2013 flood in the Upper Bow River, Canada,
Hydrol. Process., 30: 4657–4673. \Rhref{https://doi.org/10.1002/hyp.10957}{doi:10.1002\slash{}hyp.10957}.
\end{References}
\HeaderA{CAN\_07BE001}{CAN-07BE001}{CAN.Rul.07BE001}
\keyword{datasets}{CAN\_07BE001}
%
\begin{Description}
A dataframe of annual maximum series observations for
station 07BE001, ATHABASCA RIVER AT ATHABASCA in Alberta, Canada.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
CAN_07BE001
\end{verbatim}
\end{Usage}
%
\begin{Format}
A dateframe with 108 rows and 2 columns spanning the period 1913-2020.
\end{Format}
%
\begin{Details}
Variables:
\begin{itemize}

\item{} \code{max}: Numeric; the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: Integer; the corresponding year.

\end{itemize}

\end{Details}
%
\begin{Section}{Additional Information}

This is an \emph{unregulated} station that is not in the RHBN network. Other notable features include:
\begin{itemize}

\item{} The MKS/Pettitt tests find no evidence of change points at the 0.05 significance level.
\item{} Trend detection finds no evidence of trends in the mean or variability.

\end{itemize}


This dataset is used as a test case for comparison with the MATLAB implementation of the
FFA framework. It is also an excellent introductory example to stationary FFA.
\end{Section}
%
\begin{Source}
Meteorological Service of Canada (MSC) GeoMet Platform
\end{Source}
\HeaderA{CAN\_08MH016}{CAN-08MH016}{CAN.Rul.08MH016}
\keyword{datasets}{CAN\_08MH016}
%
\begin{Description}
A dataframe of annual maximum series observations for
station 08MH016, CHILLIWACK RIVER AT CHILLIWACK LAKE in British Columbia, Canada.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
CAN_08MH016
\end{verbatim}
\end{Usage}
%
\begin{Format}
A dateframe with 95 rows and 2 columns spanning the period 1922-2016.
\end{Format}
%
\begin{Details}
Variables:
\begin{itemize}

\item{} \code{max}: Numeric; the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: Integer; the corresponding year.

\end{itemize}

\end{Details}
%
\begin{Section}{Additional Information}

This is an \emph{unregulated} station in the RHBN network. Other notable features include:
\begin{itemize}

\item{} The MKS/Pettitt tests find no evidence of change points at the 0.05 significance level.
\item{} Trend detection finds evidence of an increasing trend in the variability.

\end{itemize}


This dataset is used as a test case for comparison with the MATLAB implementation of the
FFA framework. It is also useful for demonstrating how the framework detects and handles
nonstationarity in the variability of a time series.
\end{Section}
%
\begin{Source}
Meteorological Service of Canada (MSC) GeoMet Platform
\end{Source}
\HeaderA{CAN\_08NH021}{CAN-08NH021}{CAN.Rul.08NH021}
\keyword{datasets}{CAN\_08NH021}
%
\begin{Description}
A dataframe of annual maximum series observations for
station 08NH021, KOOTENAI RIVER AT PORTHILL in British Columbia, Canada.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
CAN_08NH021
\end{verbatim}
\end{Usage}
%
\begin{Format}
A dateframe with 91 rows and 2 columns spanning the period 1928-2018.
\end{Format}
%
\begin{Details}
Variables:
\begin{itemize}

\item{} \code{max}: Numeric; the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: Integer; the corresponding year.

\end{itemize}

\end{Details}
%
\begin{Section}{Additional Information}

This is a \emph{regulated} station that is not in the RHBN network. Other notable features include:
\begin{itemize}

\item{} The Libby dam was constructed upstream of this station in 1972.
\item{} The Pettitt test found evidence of a change point in 1972 at the 0.05 significance level.
\item{} The MKS test found evidence of change points in 1960 \& 1985 at the 0.05 significance level.
\item{} After splitting the data in 1972, trend detection finds evidence of an increasing,
deterministic, and linear trend in the mean for both subperiods.

\end{itemize}


This dataset is used as a test case for comparison with the MATLAB implementation of the
FFA framework. It is also useful for demonstrating how the framework detects and handles
change points in a time series.
\end{Section}
%
\begin{Source}
Meteorological Service of Canada (MSC) GeoMet Platform
\end{Source}
\HeaderA{CAN\_08NM050}{CAN-08NM050}{CAN.Rul.08NM050}
\keyword{datasets}{CAN\_08NM050}
%
\begin{Description}
A dataframe of annual maximum series observations for
station 08NM050, OKANAGAN RIVER AT PENTICTON in British Columbia, Canada.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
CAN_08NM050
\end{verbatim}
\end{Usage}
%
\begin{Format}
A dateframe with 97 rows and 2 columns spanning the period 1921-2017.
\end{Format}
%
\begin{Details}
Variables:
\begin{itemize}

\item{} \code{max}: Numeric; the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: Integer; the corresponding year.

\end{itemize}

\end{Details}
%
\begin{Section}{Additional Information}

This is a \emph{regulated} station that is not part of the RHBN. Other notable features include:
\begin{itemize}

\item{} The Okanagan River upstream of the station has been regulated since 1914 due to the
construction of the first dam, followed by a second dam in 1920, and a regulation system
in the early 1950s, consisting of four dams and 38 km of engineered channel.
\item{} Rapid human settlement, development, and agricultural activity have occurred in the watershed.

\end{itemize}


This dataset is used as a test case for comparison with the MATLAB implementation of the
FFA framework. It is also useful for demonstrating how the framework detects and handles
serial correlation, trends in the mean, and trends in the variability. As noted above,
this dataset is heavily influenced by reservoir operations and is intended for teaching
purposes—not for design flood estimation.
\end{Section}
%
\begin{Source}
Meteorological Service of Canada (MSC) GeoMet Platform
\end{Source}
\HeaderA{CAN\_08NM116}{CAN-08NM116}{CAN.Rul.08NM116}
\keyword{datasets}{CAN\_08NM116}
%
\begin{Description}
A dataframe of annual maximum series observations for
station 08NM116, MISSION CREEK NEAR EAST KELOWNA in British Columbia, Canada.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
CAN_08NM116
\end{verbatim}
\end{Usage}
%
\begin{Format}
A dateframe with 75 rows and 2 columns spanning the period 1949-2023.
\end{Format}
%
\begin{Details}
Variables:
\begin{itemize}

\item{} \code{max}: Numeric; the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: Integer; the corresponding year.

\end{itemize}

\end{Details}
%
\begin{Section}{Additional Information}

This is an unregulated station that is not part of the RHBN. Other notable features include:
\begin{itemize}

\item{} The MKS/Pettitt tests do not find evidence of change points at the 0.05 significance level.
\item{} Trend detection finds evidence of a trend in variability.

\end{itemize}


This dataset is an excellent application of the FFA framework for practitioners.
\end{Section}
%
\begin{Source}
Meteorological Service of Canada (MSC) GeoMet Platform
\end{Source}
\HeaderA{data\_decomposition}{Decompose Annual Maximum Series}{data.Rul.decomposition}
%
\begin{Description}
Decomposes the annual maxima series to derive its stationary stochastic component,
which can be used to identify a best-fit distribution using conventional stationary
methods. The decomposition procedure follows that proposed by Vidrio-Sahagún and He
(2022), which relies on the statistical representation of nonstationary stochastic
processes.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
data_decomposition(data, years, structure)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Details}
Four scenarios are supported:
\begin{enumerate}

\item{} No trend (the data is returned unmodified).
\item{} Linear trend in the mean only.
\item{} Linear trend in the standard deviation only.
\item{} Linear trends in both the mean and the standard deviation.

\end{enumerate}


Internally, the function does the following:
\begin{enumerate}

\item{} Compute \code{covariate = (years - 1900) / 100}.
\item{} If there is a trend in the location, fit Sen’s trend estimator to
\code{data} and \code{covariate}. Then, remove the fitted linear trend.
\item{} If there is a trend in the scale, compute the variability using
\code{\LinkA{data\_mw\_variability()}{data.Rul.mw.Rul.variability}}, fit Sen’s trend estimator to the vector of
standard deviations, and then rescale the series to remove trends
in the scale.
\item{} If necessary, shift the data so that its minimum is at least 1.

\end{enumerate}

\end{Details}
%
\begin{Value}
Numeric vector of decomposed data.
\end{Value}
%
\begin{References}
Vidrio-Sahagún, C. T., and He, J. (2022). The decomposition-based nonstationary
flood frequency analysis. Journal of Hydrology, 612 (September 2022), 128186.
\Rhref{https://doi.org/10.1016/j.jhydrol.2022.128186}{doi:10.1016\slash{}j.jhydrol.2022.128186}
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{data\_mw\_variability()}{data.Rul.mw.Rul.variability}}, \code{\LinkA{eda\_sens\_trend()}{eda.Rul.sens.Rul.trend}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
structure <- list(location = TRUE, scale = FALSE)
data_decomposition(data, years, structure)

\end{ExampleCode}
\end{Examples}
\HeaderA{data\_geomet}{Fetch Data from MSC GeoMet API}{data.Rul.geomet}
%
\begin{Description}
Gets annual maximum series data for a hydrological monitoring
station from the MSC GeoMet API.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
data_geomet(station_id)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{station\_id}] A character scalar containing the ID of a hydrological monitoring station.
You can search for station IDs by name, province, drainage basin, and location
\Rhref{https://wateroffice.ec.gc.ca/search/real_time_e.html}{here}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe with two columns:
\begin{itemize}

\item{} \code{max}: A float, the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: An integer, the corresponding year.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{data\_local()}{data.Rul.local}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Get data for the BOW RIVER AT BANFF (05BB001)
df <- data_geomet("05BB001")

\end{ExampleCode}
\end{Examples}
\HeaderA{data\_local}{Fetch Local Package Data}{data.Rul.local}
%
\begin{Description}
Fetch annual maximum series data for a hydrological monitoring station
from the package data directory.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
data_local(csv_file)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{csv\_file}] A character scalar containing the file name of a local dataset in
\AsIs{\texttt{/inst/extdata}}. Must be one of:
\begin{itemize}

\item{} \code{"CAN-05BA001.csv"}: BOW RIVER AT LAKE LOUISE
\item{} \code{"CAN-05BB001.csv"}: BOW RIVER AT BANFF
\item{} \code{"CAN-07BE001.csv"}: ATHABASCA RIVER AT ATHABASCA
\item{} \code{"CAN-08MH016.csv"}: CHILLIWACK RIVER AT CHILLIWACK LAKE
\item{} \code{"CAN-08NH021.csv"}: KOOTENAI RIVER AT PORTHILL
\item{} \code{"CAN-08NM050.csv"}: OKANAGAN RIVER AT PENTICTON
\item{} \code{"CAN-08NM116.csv"}: MISSION CREEK NEAR EAST KELOWNA

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe with two columns:
\begin{itemize}

\item{} \code{max}: A float, the annual maximum series observation, in m\eqn{^3}{}/s.
\item{} \code{year}: An integer, the corresponding year.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{data\_geomet()}{data.Rul.geomet}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Get data for the BOW RIVER AT BANFF (05BB001)
df <- data_local("CAN-05BB001.csv")

\end{ExampleCode}
\end{Examples}
\HeaderA{data\_mw\_variability}{Estimate Variance for Annual Maximum Series Data}{data.Rul.mw.Rul.variability}
%
\begin{Description}
Generates a time series of standard deviations using a moving window algorithm,
which can be used to explore potential evidence of nonstationarity in the
variability of a dataset. It returns a list that pairs each window’s mean year with
its window standard deviation. The hyperparameters \code{size} and \code{step} control the
behaviour of the moving window. Following the simulation findings from Vidrio-Sahagún
and He (2022), the default window size and step are set to 10 and 5 years
respectively. However, these can be changed by the user.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
data_mw_variability(data, years, size = 10L, step = 5L)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{size}] Integer scalar. The number of years in each moving window.
Must be a positive number less than or equal to \code{length(data)}
(default is 10).

\item[\code{step}] Integer scalar. The offset (in years) between successive
moving windows. Must be a positive number (default is 5).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with two entries:
\begin{itemize}

\item{} \code{years}: Numeric vector containing the mean year within each window.
\item{} \code{std}: Numeric vector of standard deviations within each window.

\end{itemize}

\end{Value}
%
\begin{References}
Vidrio-Sahagún, C. T., and He, J. (2022). The decomposition-based nonstationary
flood frequency analysis. Journal of Hydrology, 612 (September 2022), 128186.
\Rhref{https://doi.org/10.1016/j.jhydrol.2022.128186}{doi:10.1016\slash{}j.jhydrol.2022.128186}
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
data_mw_variability(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_bbmk\_test}{Block-Bootstrap Mann-Kendall Test for Trend Detection}{eda.Rul.bbmk.Rul.test}
%
\begin{Description}
Performs a bootstrapped version of the Mann-Kendall trend test to account
for serial correlation in annual maximum series data. The procedure
uses Spearman’s serial correlation test to estimate the least insignificant lag,
then applies a bootstrap procedure to obtain the empirical p-value and confidence
bounds for the Mann-Kendall S-statistic.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_bbmk_test(data, alpha = 0.05, samples = 10000L, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{samples}] Integer scalar. The number of bootstrap samples. Default is 10000.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The block size for the bootstrap is selected as \code{least\_lag + 1}, where \code{least\_lag}
is estimated using \code{\LinkA{eda\_spearman\_test()}{eda.Rul.spearman.Rul.test}}. Bootstrap samples are generated by
resampling blocks of the original data without replacement and computing the
Mann-Kendall S-statistic. This procedure removes serial correlation from the data.
\end{Details}
%
\begin{Value}
List; the results of the test, including:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{s\_bootstrap}: Vector of bootstrapped Mann-Kendall S-statistics.
\item{} \code{s\_statistic}: The Mann-Kendall S-statistic computed on the original series.
\item{} \code{p\_value}: Empirical two-sided p-value derived from the bootstrap distribution.
\item{} \code{bounds}: Confidence interval bounds for the null distribution of the statistic.
\item{} \code{reject}: Logical. If \code{TRUE}, the null hypothesis was rejected at \code{alpha}.
\item{} \code{msg}: Character string summarizing the test result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{References}
Bayazit, M., 2015. Nonstationarity of hydrological records and recent trends in trend
analysis: a state-of-the-art review. Environ. Processes 2 (3), 527–542.
\Rhref{https://doi.org/10.1007/s40710-015-0081-7}{doi:10.1007\slash{}s40710\-015\-0081\-7}

Khaliq, M.N., Ouarda, T.B.M.J., Gachon, P., Sushama, L., St-Hilaire, A., 2009.
Identification of hydrological trends in the presence of serial and cross correlations:
a review of selected methods and their application to annual flow regimes of Canadian
rivers. J. Hydrol. 368 (1–4), 117–130. \Rhref{https://doi.org/10.1016/j.jhydrol.2009.01.035}{doi:10.1016\slash{}j.jhydrol.2009.01.035}

Sonali, P., Nagesh Kumar, D., 2013. Review of trend detection methods and their
application to detect temperature changes in India. J. Hydrol. 476, 212–227.
\Rhref{https://doi.org/10.1016/j.jhydrol.2012.10.034}{doi:10.1016\slash{}j.jhydrol.2012.10.034}
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{plot\_bbmk\_test()}{plot.Rul.bbmk.Rul.test}}, \code{\LinkA{eda\_mk\_test()}{eda.Rul.mk.Rul.test}}, \code{\LinkA{eda\_spearman\_test()}{eda.Rul.spearman.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
eda_bbmk_test(data, samples = 1000L)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_kpss\_test}{Kwiatkowski–Phillips–Schmidt–Shin (KPSS) Unit Root Test}{eda.Rul.kpss.Rul.test}
%
\begin{Description}
Performs the KPSS unit root test on annual maximum series data.
The null hypothesis is that the time series is trend-stationary with a linear
trend and constant drift (and thus has a deterministic linear trend). The
alternative hypothesis is that the time series has a unit root (and thus
has a stochastic trend).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_kpss_test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The implementation of the KPSS test is based on the \pkg{aTSA} package, which
interpolates a significance table from Hobjin et al. (2004). Therefore, a result
of \eqn{p = 0.01}{} implies that \eqn{p \leq 0.01}{} and a result of \eqn{p = 0.10}{}
implies that \eqn{p \geq 0.10}{}. This implementation uses the Type III KPSS test,
which accounts for a linear trend in the data.
\end{Details}
%
\begin{Value}
A list containing the test results, including:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{statistic}: The KPSS test statistic.
\item{} \code{p\_value}: The interpolated p-value. See note regarding discrete thresholds.
\item{} \code{reject}: Logical scalar. If, TRUE the null hypothesis is rejected at \code{alpha}.
\item{} \code{msg}: Character string summarizing the test outcome, printed if \code{quiet = FALSE}.

\end{itemize}

\end{Value}
%
\begin{References}
Hobijn, B., Franses, P.H. and Ooms, M. (2004), Generalizations of the KPSS-test
for stationarity. Statistica Neerlandica, 58: 483-502.

Kwiatkowski, D.; Phillips, P. C. B.; Schmidt, P.; Shin, Y. (1992). Testing the null
hypothesis of stationarity against the alternative of a unit root. Journal of
Econometrics, 54 (1-3): 159-178.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{eda\_pp\_test()}{eda.Rul.pp.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
eda_kpss_test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_mks\_test}{Mann–Kendall–Sneyers Test for Change Point Detection}{eda.Rul.mks.Rul.test}
%
\begin{Description}
Performs the Mann–Kendall–Sneyers (MKS) test to detect the beginning of a monotonic
trend in annual maximum series data. The test computes normalized
progressive and regressive Mann–Kendall statistics and identifies statistically
significant crossing points, indicating potential change points in the trend.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_mks_test(data, years, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function computes progressive and regressive Mann–Kendall statistics \eqn{S_t}{},
normalized by their expected values and variances under the null hypothesis. The
crossing points where the difference between these normalized statistics changes
sign are identified using linear interpolation. The significance of detected
crossings is assessed using quantiles of the normal distribution.
\end{Details}
%
\begin{Value}
A list containing the test results, including:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{years}: The \code{years} argument.
\item{} \code{s\_progressive}: Normalized progressive Mann–Kendall-Sneyers statistics.
\item{} \code{s\_regressive}: Normalized regressive Mann–Kendall-Sneyers statistics.
\item{} \code{bound}: Critical confidence bound for significance based on \code{alpha}.
\item{} \code{crossing\_df}: Crossing points, including indices, years, and test statistics.
\item{} \code{change\_df}: Subset of \code{crossing\_df} with statistically significant crossings.
\item{} \code{p\_value}: Two-sided p-value derived from the maximum crossing statistic.
\item{} \code{reject}: Logical. If \code{TRUE}, the null hypothesis of no change point is rejected.
\item{} \code{msg}: Character string summarizing the test result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{References}
Sneyers, R. (1990). On the statistical analysis of series of observations.
Technical note No. 143, World Meteorological Organization, Geneva.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{plot\_mks\_test()}{plot.Rul.mks.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
eda_mks_test(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_mk\_test}{Mann–Kendall Trend Test}{eda.Rul.mk.Rul.test}
%
\begin{Description}
Performs the Mann–Kendall trend test on a numeric vector to detect the presence
of a monotonic trend (increasing or decreasing) over time. The test is
nonparametric and accounts for tied observations in the data. The null
hypothesis assumes there is no monotonic trend.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_mk_test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The statistic \eqn{S}{} is computed as the sum over all pairs \eqn{i < j}{} of the
sign of the difference \eqn{x_j - x_i}{}. Ties are explicitly accounted for when
calculating the variance of \eqn{S}{}, using grouped frequencies of tied observations.
The test statistic \eqn{Z}{} is then computed based on the sign and magnitude of
\eqn{S}{}, and the p-value is derived from the standard normal distribution.
\end{Details}
%
\begin{Value}
A list containing the test results, including:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{s\_statistic}: The Mann–Kendall test statistic \eqn{S}{}.
\item{} \code{s\_variance}: The variance of the test statistic under the null hypothesis.
\item{} \code{p\_value}: The p-value associated with the two-sided hypothesis test.
\item{} \code{reject}: Logical. If \code{TRUE}, the null hypothesis is rejected at \code{alpha}.
\item{} \code{msg}: A character string summarizing the result, printed if \code{quiet = FALSE}.

\end{itemize}

\end{Value}
%
\begin{References}
Kendall, M. (1975). Rank Correlation Methods. Griffin, London, 202 pp.

Mann, H. B. (1945). Nonparametric Tests Against Trend. Econometrica, 13(3): 245-25
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{eda\_bbmk\_test()}{eda.Rul.bbmk.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
eda_mk_test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_pettitt\_test}{Pettitt Test for Abrupt Changes in the Mean of a Time Series}{eda.Rul.pettitt.Rul.test}
%
\begin{Description}
Performs the nonparametric Pettitt test to detect a single abrupt shift in the
mean of a time series. Under the null hypothesis, there is no change point.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_pettitt_test(data, years, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The Pettitt test computes the maximum absolute value of the U-statistic
over all possible split points. The p-value is approximated using an
asymptotic formula.
\end{Details}
%
\begin{Value}
A list containing the test results, including:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{years}: The \code{years} argument.
\item{} \code{u\_t}: Numeric vector of absolute U-statistics for all time indices.
\item{} \code{k\_statistic}: Numeric scalar. The maximum absolute U-statistic.
\item{} \code{k\_critical}: Numeric scalar. The critical K-statistic value for given \code{alpha}.
\item{} \code{p\_value}: Numeric scalar. Approximate p-value for the test.
\item{} \code{change\_index}: Integer scalar. Index of the detected change point (0 if none).
\item{} \code{change\_year}: Integer scalar. Year of the detected change point (0 if none).
\item{} \code{reject}: Logical scalar. If \code{TRUE}, the null hypothesis was rejected.
\item{} \code{msg}: Character scalar. A formatted summary message describing the test result.

\end{itemize}

\end{Value}
%
\begin{References}
Pettitt, A.N., 1979. A Non-parametric Approach to the Change-point Problem. J.
Royal Statist. Soc. 28 (2), 126–135. \url{http://www.jstor.org/stable/2346729}
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{plot\_pettitt\_test()}{plot.Rul.pettitt.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
eda_pettitt_test(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_pp\_test}{Phillips–Perron Unit Root Test}{eda.Rul.pp.Rul.test}
%
\begin{Description}
Applies the Phillips–Perron (PP) test to check for a unit root in annual
maximum series data. The null hypothesis assumes the time series contains a
unit root (and thus has a stochastic trend). The alternative hypothesis is that
the time series is trend-stationary (and thus has a deterministic linear trend).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_pp_test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The implementation of this test is based on the \pkg{aTSA} package, which
interpolates p-values from a table of critical values presented in Fuller W. A.
(1996). The critical values are only available for \eqn{\alpha \geq 0.01}{}.
Therefore, a reported p-value of 0.01 indicates \eqn{p \leq 0.01}{}.
\end{Details}
%
\begin{Value}
List; the test results, consisting of:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{statistic}: The Z-statistic used to perform the test.
\item{} \code{p\_value}: Reported p-value from the test. See notes on interpolation thresholds.
\item{} \code{reject}: Logical. If \code{TRUE}, the null hypothesis was rejected at \code{alpha}.
\item{} \code{msg}: Character string summarizing the test result, printed if \code{quiet = FALSE}.

\end{itemize}

\end{Value}
%
\begin{References}
Fuller, W. A. (1976). Introduction to Statistical Time Series. New York:
John Wiley and Sons

Phillips, P. C. B.; Perron, P. (1988). Testing for a Unit Root in Time Series
Regression. Biometrika, 75 (2): 335-346
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{eda\_kpss\_test()}{eda.Rul.kpss.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
eda_pp_test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_runs\_test}{Wald–Wolfowitz Runs Test for Randomness}{eda.Rul.runs.Rul.test}
%
\begin{Description}
Applies the Wald–Wolfowitz runs test to a numeric vector of residuals in
order to assess whether they behave as a random sequence. The test statistic’s
p-value is compared to the significance level \code{alpha}, and a decision is
returned along with a human-readable summary message.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_runs_test(results, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A fitted linear model produced by \code{\LinkA{eda\_sens\_trend()}{eda.Rul.sens.Rul.trend}}.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The Wald–Wolfowitz runs test examines the sequence of residuals to test for
randomness around the median. A small p-value suggests nonrandom clustering,
which may indicate that a linear model is inappropriate for the data.
\end{Details}
%
\begin{Value}
A list containing the test results, including:
\begin{itemize}

\item{} \code{residuals}: Numeric vector of residual values from a fitted linear model.
\item{} \code{n}: The length of the residuals vector after removing the median.
\item{} \code{n\_plus}: The number of residuals above the median.
\item{} \code{n\_minus}: The number of residuals below the median.
\item{} \code{runs}: The number of runs in the transformed sequence of residuals.
\item{} \code{statistic}: The runs test statistic, computed using \code{runs}.
\item{} \code{p\_value}: P-value from the Wald–Wolfowitz runs test applied to residuals.
\item{} \code{reject}: Logical. If TRUE, the null hypothesis of random residuals is rejected.
\item{} \code{msg}: Character string summarizing the test result, printed if \code{quiet = FALSE}.

\end{itemize}

\end{Value}
%
\begin{References}
Wald, A. and Wolfowitz, J. (1940). On a test whether two samples are from the
same population. Annals of Mathematical Statistics, 11(2), 147–162.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{plot\_runs\_test()}{plot.Rul.runs.Rul.test}}, \code{\LinkA{eda\_sens\_trend()}{eda.Rul.sens.Rul.trend}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
sens <- eda_sens_trend(data, years)
eda_runs_test(sens)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_sens\_trend}{Sen's Trend Estimator}{eda.Rul.sens.Rul.trend}
%
\begin{Description}
Computes Sen's linear trend estimator for a univariate time series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_sens_trend(data, years, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Sen's slope estimator is a robust, nonparametric trend estimator based on the
median of all pairwise slopes between data points. The corresponding intercept
is the median of each \eqn{y_i - mx_i}{} where \eqn{m}{} is the estimated slope.
\end{Details}
%
\begin{Value}
A list containing the estimated trend:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{years}: The \code{years} argument.
\item{} \code{slope}: Median slope of all pairwise data-year combinations.
\item{} \code{intercept}: Median intercept estimate of the fitted line.
\item{} \code{residuals}: Vector of residuals between observed and fitted values.
\item{} \code{msg}: Character string summarizing the results.

\end{itemize}

\end{Value}
%
\begin{References}
Sen, P.K. (1968). Estimates of the regression coefficient based on Kendall's tau.
\emph{Journal of the American Statistical Association}, 63(324), 1379–1389.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{eda\_runs\_test()}{eda.Rul.runs.Rul.test}}, \code{\LinkA{plot\_sens\_trend()}{plot.Rul.sens.Rul.trend}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
eda_sens_trend(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_spearman\_test}{Spearman Test for Autocorrelation}{eda.Rul.spearman.Rul.test}
%
\begin{Description}
Performs the Spearman rank serial correlation test on annual maximum series
data to check for serial correlation at various lags. Reports the smallest
lag where the serial correlation is not statistically significant at the given
significance level.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_spearman_test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
For each lag from \eqn{1}{} to \eqn{n - 3}{}, the function computes the Spearman
correlation coefficient between the original series and the lagged series. The
first lag with an insignificant serial correlation coefficient returned as \code{least\_lag}.
\end{Details}
%
\begin{Value}
A list containing the test results, including:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{rho}: Numeric vector of serial correlation estimates for lags \eqn{1}{} to \eqn{n-3}{}.
\item{} \code{sig}: Logical vector indicating which lags exhibit significant serial correlation
\item{} \code{least\_lag}: The smallest lag at which the serial correlation is insignificant.
\item{} \code{reject}: Logical. If \code{TRUE}, then \code{least\_lag > 0}.
\item{} \code{msg}: Character string summarizing the test result, printed if \code{quiet = FALSE}.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{stats::cor.test()}{stats::cor.test()}}, \code{\LinkA{eda\_bbmk\_test()}{eda.Rul.bbmk.Rul.test}}, \code{\LinkA{plot\_spearman\_test()}{plot.Rul.spearman.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
eda_spearman_test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{eda\_white\_test}{White Test for Heteroskedasticity}{eda.Rul.white.Rul.test}
%
\begin{Description}
Performs the White test for heteroskedasticity by regressing the squared residuals
of a linear model on the original regressors and their squared terms. The null
hypothesis is homoskedasticity.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eda_white_test(data, years, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{quiet}] Logical scalar. If \code{FALSE}, prints a summary of of the statistical
test to the console. Default is \code{TRUE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The White test regresses the squared residuals from a primary linear model
\code{lm(data \textasciitilde{} years)} against both the original regressor and its square.
The test statistic is calculated as \eqn{nR^2}{}, where \eqn{R^2}{} is the
coefficient of determination from the auxiliary regression. Under the null
hypothesis, the test statistic has the \eqn{\chi^2}{} distribution.
\end{Details}
%
\begin{Value}
A list containing the results of the White test:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{years}: The \code{years} argument.
\item{} \code{r\_squared}: Coefficient of determination from the auxiliary regression.
\item{} \code{statistic}: White test statistic based on sample size and \code{r\_squared}.
\item{} \code{p\_value}: The p-value derived from a Chi-squared distribution with \code{df = 2}.
\item{} \code{reject}: Logical. If \code{TRUE}, the null hypothesis is rejected at \code{alpha}.
\item{} \code{msg}: Character string summarizing the test result, printed if \code{quiet = FALSE}.

\end{itemize}

\end{Value}
%
\begin{References}
White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and
a direct test for heteroskedasticity. \emph{Econometrica}, 48(4), 817–838.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{stats::lm()}{stats::lm()}}, \code{\LinkA{stats::pchisq()}{stats::pchisq()}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
eda_white_test(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{fit\_lmom\_fast}{Helper Function for L-moments Parameter Estimation}{fit.Rul.lmom.Rul.fast}
%
\begin{Description}
A helper function used by \code{\LinkA{fit\_lmom\_xxx()}{fit.Rul.lmom.Rul.xxx}}.
This function does not validate parameters and is designed for use in other methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fit_lmom_fast(data, distribution)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the results of parameter estimation:
\begin{itemize}

\item{} \code{method}: \code{"L-moments"}.
\item{} \code{params}: numeric vector of 2 or 3 parameters depending on the distribution.

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{lmom\_sample()}{lmom.Rul.sample}}, \code{\LinkA{fit\_lmom\_kappa()}{fit.Rul.lmom.Rul.kappa}}, \code{\LinkA{fit\_lmom\_xxx()}{fit.Rul.lmom.Rul.xxx}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
fit_lmom_fast(data, "PE3")

\end{ExampleCode}
\end{Examples}
\HeaderA{fit\_lmom\_kappa}{L-Moments Parameter Estimation for the Kappa Distribution}{fit.Rul.lmom.Rul.kappa}
%
\begin{Description}
This functions estimates the parameters of the four-parameter Kappa distribution
using the method of L-moments. Since there is no known closed form solution for
the parameters in terms of the L-moments, the parameters are computed numerically
using Newton-Raphson iteration.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fit_lmom_kappa(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
First, the sample L-moments of the data are computed using the \code{\LinkA{lmom\_sample()}{lmom.Rul.sample}}
method. Then, the \code{\LinkA{stats::optim()}{stats::optim()}} function is used to determine the parameters
by minimizing the euclidian distance between the sample and theoretical L-moment
ratios. The implementation of this routine is based on the deprecated \code{homtest}
package, formerly avilable at \url{https://CRAN.R-project.org/package=homtest}.
\end{Details}
%
\begin{Value}
A list containing the results of parameter estimation:
\begin{itemize}

\item{} \code{method}: \code{"L-moments"}.
\item{} \code{params}: numeric vector of 4 parameters in the order location, scale, shape (2).

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{lmom\_sample()}{lmom.Rul.sample}}, \code{\LinkA{fit\_lmom\_fast()}{fit.Rul.lmom.Rul.fast}}, \code{\LinkA{fit\_lmom\_xxx()}{fit.Rul.lmom.Rul.xxx}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
fit_lmom_kappa(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{fit\_lmom\_xxx}{Parameter Estimation with L-Moments}{fit.Rul.lmom.Rul.xxx}
\aliasA{fit\_lmom\_gev}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.gev}
\aliasA{fit\_lmom\_glo}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.glo}
\aliasA{fit\_lmom\_gno}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.gno}
\aliasA{fit\_lmom\_gum}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.gum}
\aliasA{fit\_lmom\_lno}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.lno}
\aliasA{fit\_lmom\_lp3}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.lp3}
\aliasA{fit\_lmom\_nor}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.nor}
\aliasA{fit\_lmom\_pe3}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.pe3}
\aliasA{fit\_lmom\_wei}{fit\_lmom\_xxx}{fit.Rul.lmom.Rul.wei}
%
\begin{Description}
Estimate the parameters of nine different distributions (\code{"GUM"}, \code{"NOR"},
\code{"LNO"}, \code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, and \code{"WEI"}) using
the method of L-moments.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fit_lmom_gum(data)

fit_lmom_nor(data)

fit_lmom_lno(data)

fit_lmom_gev(data)

fit_lmom_glo(data)

fit_lmom_gno(data)

fit_lmom_pe3(data)

fit_lmom_lp3(data)

fit_lmom_wei(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
First, the sample L-moments of the data are computed using the \code{\LinkA{lmom\_sample()}{lmom.Rul.sample}}
method. Then formulas from Hosking (1997) are used to compute the parameters from
the L-moments. Distributions \code{"GNO"}, \code{"PE3"}, and \code{"LP3"} use a rational
approximation of the parameters.
\end{Details}
%
\begin{Value}
A list containing the results of parameter estimation:
\begin{itemize}

\item{} \code{method}: \code{"L-moments"}.
\item{} \code{params}: numeric vector of 2 or 3 parameters depending on the distribution.

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{fit\_lmom\_fast()}{fit.Rul.lmom.Rul.fast}}, \code{\LinkA{fit\_lmom\_kappa()}{fit.Rul.lmom.Rul.kappa}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
fit_lmom_lp3(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{fit\_maximum\_likelihood}{Maximum Likelihood Parameter Estimation}{fit.Rul.maximum.Rul.likelihood}
%
\begin{Description}
Estimates parameters of a probability distribution with an optional nonstationary
structure by  maximizing the log‐likelihood. Initial values are obtained through
L‐moment parameter estimation, and optimization is performed via \code{\LinkA{stats::nlminb()}{stats::nlminb()}}
with repeated perturbations if needed.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fit_maximum_likelihood(
  data,
  distribution,
  prior = NULL,
  years = NULL,
  structure = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{prior}] Numeric vector of length 2. Specifies the Beta prior shape
parameters \eqn{(p, q)}{} for the shape parameter \eqn{\kappa}{}.
Only used when \code{distribution = "GEV"}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Details}
\begin{enumerate}

\item{} Calls \code{\LinkA{fit\_lmom\_fast()}{fit.Rul.lmom.Rul.fast}} on \code{data} to obtain initial parameter estimates.
\item{} Initializes trend parameters to zero if necessary.
\item{} For \code{WEI} models, sets the location parameter to zero to ensure support.
\item{} Defines an objective function using \code{\LinkA{loglik\_fast()}{loglik.Rul.fast}} or \code{\LinkA{general\_loglik\_fast()}{general.Rul.loglik.Rul.fast}}.
\item{} Runs \code{\LinkA{stats::nlminb()}{stats::nlminb()}} with box constraints. Attempts optimization
up to 100 times if a maximum cannot be found.

\end{enumerate}

\end{Details}
%
\begin{Value}
A list containing the results of parameter estimation:
\begin{itemize}

\item{} \code{params}: Numeric vector of estimated parameters.
\item{} \code{mll}: Maximum log‐likelihood value.

\end{itemize}

\end{Value}
%
\begin{Note}
Although the more modern \code{\LinkA{stats::optim()}{stats::optim()}} function is preferred over
\code{\LinkA{stats::nlminb()}{stats::nlminb()}}, we use \code{\LinkA{stats::nlminb()}{stats::nlminb()}} because it supports infinite
values of the likelihood function.
\end{Note}
%
\begin{SeeAlso}
\code{\LinkA{loglik\_fast()}{loglik.Rul.fast}}, \code{\LinkA{general\_loglik\_fast()}{general.Rul.loglik.Rul.fast}}, \code{\LinkA{fit\_lmom\_fast()}{fit.Rul.lmom.Rul.fast}},
\code{\LinkA{stats::nlminb()}{stats::nlminb()}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
structure <- list(location = TRUE, scale = FALSE)
fit_maximum_likelihood(data, "GNO", NULL, years, structure)

\end{ExampleCode}
\end{Examples}
\HeaderA{general\_loglik\_fast}{Generalized Log-Likelihood Helper Function}{general.Rul.loglik.Rul.fast}
%
\begin{Description}
A helper function used by \code{\LinkA{general\_loglik\_gev()}{general.Rul.loglik.Rul.gev}}.
This function does not validate parameters and is designed for use in other methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
general_loglik_fast(data, distribution, params, prior, years, structure)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.

\item[\code{prior}] Numeric vector of length 2. Specifies the Beta prior shape
parameters \eqn{(p, q)}{} for the shape parameter \eqn{\kappa}{}.
Only used when \code{distribution = "GEV"}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Value}
Numeric scalar. The generalized log-likelihood value.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{general\_loglik\_gev()}{general.Rul.loglik.Rul.gev}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data, params, prior, years, and structure
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(0, 1, 1, 0, 0)
prior <- c(5, 10)
years <- seq(from = 1901, to = 2000)
structure <- list(location = TRUE, scale = TRUE)

# Compute the generalized log-likelihood
general_loglik_fast(data, "GEV", params, prior, years, structure)

\end{ExampleCode}
\end{Examples}
\HeaderA{general\_loglik\_gev}{Generalized Log-Likelihood Functions for GEV Models}{general.Rul.loglik.Rul.gev}
%
\begin{Description}
Computes the generalized log-likelihood for stationary and nonstationary
variants of the Generalized Extreme Value (GEV) distribution with a geophysical
(Beta) prior distribution for the shape parameter (Martins and Stedinger, 2000).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
general_loglik_gev(data, params, prior, years = NULL, structure = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.

\item[\code{prior}] Numeric vector of length 2. Specifies the Beta prior shape
parameters \eqn{(p, q)}{} for the shape parameter \eqn{\kappa}{}.
Only used when \code{distribution = "GEV"}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Details}
The generalized log-likelihood is defined as sum of the log-likelihood of the
specified model and the log-density of the Beta prior with parameters \eqn{(p, q)}{}.
The contribution of the prior is: \deqn{\log \pi(\kappa) = (p-1) \log(0.5-\kappa) 
+ (q-1) \log(0.5+\kappa) - \log (\beta(p, q))}{}
\end{Details}
%
\begin{Value}
Numeric scalar. The generalized log-likelihood value.
\end{Value}
%
\begin{References}
El Adlouni, S., Ouarda, T.B.M.J., Zhang, X., Roy, R., Bob´ee, B., 2007. Generalized
maximum likelihood estimators for the nonstationary generalized extreme value
model. Water Resour. Res. 43 (3), 1–13. \Rhref{https://doi.org/10.1029/2005WR004545}{doi:10.1029\slash{}2005WR004545}

Martins, E. S., and Stedinger, J. R. (2000). Generalized maximum-likelihood generalized
extreme-value quantile estimators for hydrologic data. Water Resources Research, 36(3),
737–744. \Rhref{https://doi.org/10.1029/1999WR900330}{doi:10.1029\slash{}1999WR900330}
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{general\_loglik\_fast()}{general.Rul.loglik.Rul.fast}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data, params, prior, years, and structure
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(0, 1, 1, 0)
prior <- c(5, 10)
years <- seq(from = 1901, to = 2000)
structure <- list(location = TRUE, scale = FALSE)

# Compute the generalized log-likelihood
general_loglik_gev(data, params, prior, years, structure)

\end{ExampleCode}
\end{Examples}
\HeaderA{lmom\_fast}{Helper Function for L-moments Ratios}{lmom.Rul.fast}
%
\begin{Description}
A helper function used by \code{\LinkA{lmom\_theoretical()}{lmom.Rul.theoretical}}.
This function does not validate parameters and is designed for use in other methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmom_fast(distribution, params)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric vector of with four elements:
\begin{itemize}

\item{} \eqn{\lambda_1}{}: L-mean
\item{} \eqn{\lambda_2}{}: L-variance
\item{} \eqn{\tau_3}{}: L-skewness
\item{} \eqn{\tau_4}{}: L-kurtosis

\end{itemize}

\end{Value}
%
\begin{Note}
This function returns identical L-moment ratios for \code{"NOR"}/\code{"LNO"} and
\code{"PE3"}/\code{"LP3"} since L-moments for the \code{"LNO"} and \code{"LP3"} distributions are
compared with the sample L-moments of log-transformed data internally.
\end{Note}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{lmom\_theoretical()}{lmom.Rul.theoretical}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
lmom_fast("GLO", c(0, 1, 0))

\end{ExampleCode}
\end{Examples}
\HeaderA{lmom\_sample}{Sample L-moments}{lmom.Rul.sample}
%
\begin{Description}
Computes the first four sample L-moments and L-moment ratios from a numeric
vector of data. L-moments are linear combinations of order statistics that
provide robust alternatives to conventional moments, with advantages in
parameter estimation for heavy-tailed and skewed distributions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmom_sample(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Given probability weighted moments \eqn{\beta_0, \beta_1, \beta_2, \beta_3}{},
the first four sample L-moments are:
\begin{itemize}

\item{} \eqn{l_1 = \beta_0}{}
\item{} \eqn{l_2 = 2\beta_1 - \beta_0}{}
\item{} \eqn{l_3 = 6\beta_2 - 6\beta_1 + \beta_0}{}
\item{} \eqn{l_4 = 20\beta_3 - 30\beta_2 + 12\beta_1 - \beta_0}{}

\end{itemize}


Then, the sample L-skewness is \eqn{t_3 = l_3 / l_2}{} and the sample L-kurtosis
is \eqn{t_4 = l_4 / l_2}{}.
\end{Details}
%
\begin{Value}
A numeric vector containing the first four sample L-moments and L-moment ratios:
\begin{itemize}

\item{} \eqn{l_1}{}: L-mean
\item{} \eqn{l_2}{}: L-variance
\item{} \eqn{t_3}{}: L-skewness
\item{} \eqn{t_4}{}: L-kurtosis

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J. R. M. (1990). L-moments: Analysis and estimation of distributions
using linear combinations of order statistics. \emph{Journal of the Royal Statistical
Society: Series B (Methodological)}, 52(1), 105–124.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
lmom_sample(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{lmom\_theoretical}{Theoretical L-moments of Probability Distributions}{lmom.Rul.theoretical}
\aliasA{lmom\_theoretical\_gev}{lmom\_theoretical}{lmom.Rul.theoretical.Rul.gev}
\aliasA{lmom\_theoretical\_glo}{lmom\_theoretical}{lmom.Rul.theoretical.Rul.glo}
\aliasA{lmom\_theoretical\_gno}{lmom\_theoretical}{lmom.Rul.theoretical.Rul.gno}
\aliasA{lmom\_theoretical\_gum}{lmom\_theoretical}{lmom.Rul.theoretical.Rul.gum}
\aliasA{lmom\_theoretical\_nor}{lmom\_theoretical}{lmom.Rul.theoretical.Rul.nor}
\aliasA{lmom\_theoretical\_pe3}{lmom\_theoretical}{lmom.Rul.theoretical.Rul.pe3}
\aliasA{lmom\_theoretical\_wei}{lmom\_theoretical}{lmom.Rul.theoretical.Rul.wei}
%
\begin{Description}
Computes the first four L-moments and L-moment ratios of seven different probability
distributions (\code{"GUM"}, \code{"NOR"}, \code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, and \code{"WEI"})
given the parameters of the distribution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmom_theoretical_gum(params)

lmom_theoretical_nor(params)

lmom_theoretical_gev(params)

lmom_theoretical_glo(params)

lmom_theoretical_gno(params)

lmom_theoretical_pe3(params)

lmom_theoretical_wei(params)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The distributions \code{"GUM"}, \code{"NOR"}, \code{"GEV"}, \code{"GLO"}, and \code{"WEI"} have
closed-form solutions for the L-moments and L-moment ratios in terms of the parameters.
The distributions \code{"GNO"} and \code{"PE3"} use rational approximations of the L-moment ratios
from Hosking (1997).
\end{Details}
%
\begin{Value}
A numeric vector of with four elements:
\begin{itemize}

\item{} \eqn{\lambda_1}{}: L-mean
\item{} \eqn{\lambda_2}{}: L-variance
\item{} \eqn{\tau_3}{}: L-skewness
\item{} \eqn{\tau_4}{}: L-kurtosis

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{lmom\_fast()}{lmom.Rul.fast}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
lmom_theoretical_gev(c(0, 1, 0))

\end{ExampleCode}
\end{Examples}
\HeaderA{loglik\_fast}{Log-Likelihood Helper Function}{loglik.Rul.fast}
%
\begin{Description}
A helper function used by \code{\LinkA{loglik\_xxx()}{loglik.Rul.xxx}}.
This function does not validate parameters and is designed for use in other methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
loglik_fast(data, distribution, params, years, structure)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Value}
Numeric scalar. The log-likelihood value.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{loglik\_xxx()}{loglik.Rul.xxx}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(0, 1, 0)
years <- seq(from = 1901, to = 2000)
structure <- list(location = FALSE, scale = FALSE)
loglik_fast(data, "GEV", params, years, structure)

\end{ExampleCode}
\end{Examples}
\HeaderA{loglik\_xxx}{Log-Likelihood Functions for Probability Models}{loglik.Rul.xxx}
\aliasA{loglik\_gev}{loglik\_xxx}{loglik.Rul.gev}
\aliasA{loglik\_glo}{loglik\_xxx}{loglik.Rul.glo}
\aliasA{loglik\_gno}{loglik\_xxx}{loglik.Rul.gno}
\aliasA{loglik\_gum}{loglik\_xxx}{loglik.Rul.gum}
\aliasA{loglik\_lno}{loglik\_xxx}{loglik.Rul.lno}
\aliasA{loglik\_lp3}{loglik\_xxx}{loglik.Rul.lp3}
\aliasA{loglik\_nor}{loglik\_xxx}{loglik.Rul.nor}
\aliasA{loglik\_pe3}{loglik\_xxx}{loglik.Rul.pe3}
\aliasA{loglik\_wei}{loglik\_xxx}{loglik.Rul.wei}
%
\begin{Description}
Compute the log-likelihood value for stationary and nonstationary variants
of nine different distributions: \code{"GUM"}, \code{"NOR"}, \code{"LNO"}, \code{"GEV"}, \code{"GLO"},
\code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}. In total, these methods compute the
log-likelihood for 36 different probability models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
loglik_gum(data, params, years = NULL, structure = NULL)

loglik_nor(data, params, years = NULL, structure = NULL)

loglik_lno(data, params, years = NULL, structure = NULL)

loglik_gev(data, params, years = NULL, structure = NULL)

loglik_glo(data, params, years = NULL, structure = NULL)

loglik_gno(data, params, years = NULL, structure = NULL)

loglik_pe3(data, params, years = NULL, structure = NULL)

loglik_lp3(data, params, years = NULL, structure = NULL)

loglik_wei(data, params, years = NULL, structure = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Value}
Numeric scalar. The log-likelihood value.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{loglik\_fast()}{loglik.Rul.fast}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data, params, years, and structure
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(0, 1, 1, 0)
years <- seq(from = 1901, to = 2000)
structure <- list(location = TRUE, scale = FALSE)

# Compute the log-likelihood
loglik_gno(data, params, years, structure)

\end{ExampleCode}
\end{Examples}
\HeaderA{model\_diagnostics}{Evaluate Goodness-of-Fit for Fitted Flood Models}{model.Rul.diagnostics}
%
\begin{Description}
Computes multiple performance metrics and diagnostic indicators to assess the
quality of a fitted flood frequency model. This includes accuracy (residual
statistics), fitting efficiency (information criteria), and uncertainty
(coverage-based metrics using confidence intervals).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
model_diagnostics(
  data,
  distribution,
  params,
  uncertainty,
  years = NULL,
  structure = NULL,
  alpha = 0.05,
  pp_formula = "Weibull"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.

\item[\code{uncertainty}] List; estimated reutrn levels and confidence intervals
generated by \code{\LinkA{uncertainty\_bootstrap()}{uncertainty.Rul.bootstrap}} or \code{\LinkA{uncertainty\_rfpl()}{uncertainty.Rul.rfpl}}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}


\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{pp\_formula}] Character (1); string specifying the plotting position formula.
Must be one of \code{"Weibull"}, \code{"Blom"}, \code{"Cunnane"}, \code{"Gringorten"}, or \code{"Hazen"}
\end{ldescription}
\end{Arguments}
%
\begin{Value}
List containing model assessment metrics:
\begin{itemize}

\item{} \code{data}: The \code{data} argument.
\item{} \code{estimates}: Quantile estimates for empirical return periods.
\item{} \code{R2}: Coefficient of determination from linear regression of estimates vs. data.
\item{} \code{RMSE}: Root mean squared error of quantile estimates.
\item{} \code{bias}: Mean bias of quantile estimates.
\item{} \code{AIC}: Akaike Information Criterion.
\item{} \code{BIC}: Bayesian Information Criterion.
\item{} \code{AIC\_MLL}: Akaike Information Criterion, computed using MLE.
\item{} \code{BIC\_MLL}: Bayesian Information Criterion, computed using MLE.
\item{} \code{AW}: Average width of the confidence interval(s).
\item{} \code{POC}: Percent of observations covered by the confidence interval(s).
\item{} \code{CWI}: Confidence width index, a metric that combines \code{AW} and \code{POC}.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{uncertainty\_bootstrap()}{uncertainty.Rul.bootstrap}}, \code{\LinkA{uncertainty\_rfpl()}{uncertainty.Rul.rfpl}}, \code{\LinkA{fit\_maximum\_likelihood()}{fit.Rul.maximum.Rul.likelihood}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize example data and params
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(100, 10)

# Perform uncertainty analysis
uncertainty <- uncertainty_bootstrap(data, "NOR", "L-moments")

# Evaluate model diagnostics
model_diagnostics(data, "NOR", params, uncertainty)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_ams\_data}{Plot Annual Maximum Series Data}{plot.Rul.ams.Rul.data}
%
\begin{Description}
Generates a plot of annual maximum series data with an optional line connecting
the data points.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_ams_data(data, years, show_line = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{show\_line}] If \code{TRUE} (default), draw a line through the data.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; A plot containing:
\begin{itemize}

\item{} Black dots at each (data, year) pair.
\item{} An optional line thorugh the data points (if \code{show\_line == TRUE})

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
plot_ams_data(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_bbmk\_test}{Plot Block‐Bootstrap Mann–Kendall Test Results}{plot.Rul.bbmk.Rul.test}
%
\begin{Description}
Generates a histogram of bootstrapped Mann–Kendall S‐statistics with vertical
lines indicating the observed S‐statistic and confidence bounds.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_bbmk_test(results, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] List of BB‐MK test results generated by \code{\LinkA{eda\_bbmk\_test()}{eda.Rul.bbmk.Rul.test}}.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; A plot containing:
\begin{itemize}

\item{} A gray histogram of the distribution of bootstrapped S‐statistics.
\item{} A red vertical line at the lower and upper confidence bounds.
\item{} A black vertical line at the observed S‐statistic.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
results <- eda_bbmk_test(data, samples = 1000L)
plot_bbmk_test(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_lmom\_diagram}{Plot L-Moment Ratio Diagram}{plot.Rul.lmom.Rul.diagram}
%
\begin{Description}
Generates a plot of L-moment ratios with the L-skewness on the x-axis and L-kurtosis
on the y-axis. Plots the sample and log-sample L-moment ratios alongside the
theoretical L-moment ratios for a set of candidate distributions. Also includes
a small inset around the L-moment ratios of the recommended distribution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_lmom_diagram(results, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] List of distribution selection results generated by \code{\LinkA{select\_ldistance()}{select.Rul.ldistance}},
\code{\LinkA{select\_lkurtosis()}{select.Rul.lkurtosis}}, or \code{\LinkA{select\_zstatistic()}{select.Rul.zstatistic}}.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; plot object containing the L-moment ratio diagram, with:
\begin{itemize}

\item{} L-moment ratio curves for each 3-parameter distribution.
\item{} Points for the L-moment ratios of each 2-parameter distribution.
\item{} Sample and log-sample L-moment ratio \eqn{(t_3, t_4)}{} points.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{select\_ldistance()}{select.Rul.ldistance}}, \code{\LinkA{select\_lkurtosis()}{select.Rul.lkurtosis}}, \code{\LinkA{select\_zstatistic()}{select.Rul.zstatistic}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
results <- select_ldistance(data)
plot_lmom_diagram(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_mks\_test}{Plot Mann–Kendall–Sneyers (MKS) Test Results}{plot.Rul.mks.Rul.test}
%
\begin{Description}
Constructs a two‐panel visualization of the MKS test. The upper panel plots the
normalized progressive and regressive Mann–Kendall S‐statistics over time, with
dashed confidence bounds and potential trend‐change points. The lower panel
contains the annual maximum series data with the change points highlighted.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_mks_test(results, show_line = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A list generated by \code{\LinkA{eda\_mks\_test()}{eda.Rul.mks.Rul.test}}.

\item[\code{show\_line}] If \code{TRUE} (default), draw a fitted line through the data.

\item[\code{...}] Optional named arguments: 'title', 'top\_xlabel', 'top\_ylabel',
'bottom\_xlabel' and 'bottom\_ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A \code{patchwork} object with two \code{ggplot2} panels stacked vertically.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{eda\_mks\_test()}{eda.Rul.mks.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
results <- eda_mks_test(data, years)
plot_mks_test(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_model\_diagnostics}{Plot Model Assessment Results}{plot.Rul.model.Rul.diagnostics}
%
\begin{Description}
Creates a quantile–quantile plot comparing observed annual maximum series
data to quantile estimates from a fitted parametric model. The 1:1 line is drawn in
black and the parametric model estimates are plotted as semi‐transparent red points.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_model_diagnostics(results, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] List; model assessment results generated by \code{\LinkA{model\_diagnostics()}{model.Rul.diagnostics}}.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot containing:
\begin{itemize}

\item{} A black line representing a model with no deviation from the emprical quantiles.
\item{} Red points denoting the estimated quantiles against the empirical quantiles.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize example data and params
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(100, 10)

# Perform uncertainty analysis
uncertainty <- uncertainty_bootstrap(data, "NOR", "L-moments")

# Evaluate model diagnostics
results <- model_diagnostics(data, "NOR", params, uncertainty)

# Generate a model assessment plot
plot_model_diagnostics(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_nsffa}{Plot Return Levels and Confidence Intervals for NS-FFA}{plot.Rul.nsffa}
%
\begin{Description}
Generates a plot with effective return periods on the x-axis and effective return
levels (annual maxima magnitudes) on the y-axis for up to 5 time slices. Each slice
is displayed in a distinct color. Confidence bounds are shown as semi-transparent
ribbons, and the point estimates  are overlaid as solid lines. Return periods are
depicted on a logarithmic scale.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_nsffa(results, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A list of estimated return levels and confidence intervals
generated by \code{\LinkA{uncertainty\_bootstrap()}{uncertainty.Rul.bootstrap}} or \code{\LinkA{uncertainty\_rfpl()}{uncertainty.Rul.rfpl}}.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot with one line and ribbon per slice.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}

data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)

# Run the uncertainty bootstrap at slices 1920, 1960, 2000
results <- uncertainty_bootstrap(
	   data,
   "GEV",
   "L-moments",
   years = years,
   structure = list(location = TRUE, scale = FALSE),
   slices = c(1920, 1960, 2000),
	   samples = 1000L
)

# Generate the plot
plot_nsffa(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_pettitt\_test}{Plot Results from the Pettitt Change‐Point Test}{plot.Rul.pettitt.Rul.test}
%
\begin{Description}
Creates a two‐panel visualization of the Mann–Whitney–Pettitt test. The
upper panel plots the Pettitt \eqn{U_t}{} statistic over time along with the
significance threshold and potential change point. The lower panel displays
the annual maximum series data with an optional trend line, the period
mean(s), and potential change point(s).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_pettitt_test(results, show_line = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A list generated by \code{\LinkA{eda\_pettitt\_test()}{eda.Rul.pettitt.Rul.test}}.

\item[\code{show\_line}] If \code{TRUE} (default), draw a fitted line through the data.

\item[\code{...}] Optional named arguments: 'title', 'top\_xlabel', 'top\_ylabel',
'bottom\_xlabel' and 'bottom\_ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A \code{patchwork} object with two \code{ggplot2} panels stacked vertically.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{eda\_pettitt\_test()}{eda.Rul.pettitt.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
results <- eda_pettitt_test(data, years)
plot_pettitt_test(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_runs\_test}{Plot Runs Test Results}{plot.Rul.runs.Rul.test}
%
\begin{Description}
Generates a residual plot of Sen’s estimator applied to annual maximum series
data (or the variability of the data) with a horizontal dashed line at
zero and an annotation indicating the p-value of the Runs test.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_runs_test(results, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A list of runs test results generated by \code{\LinkA{eda\_runs\_test()}{eda.Rul.runs.Rul.test}}.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot containing:
\begin{itemize}

\item{} Black points for the residual at each year.
\item{} A red dashed horizontal line at \eqn{y = 0}{}.
\item{} A text annotation “Runs p-value: X.XXX” in the plot area.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{eda\_runs\_test()}{eda.Rul.runs.Rul.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data and years
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)

# Generate runs test plot 
sens <- eda_sens_trend(data, years)
results <- eda_runs_test(sens)
plot_runs_test(results, "mean")

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_sens\_trend}{Plot Sen’s Trend Estimator}{plot.Rul.sens.Rul.trend}
%
\begin{Description}
Produces a scatterplot of the annual maximum series data or its variance
against time, optionally overlaid with Sen’s trend estimator of the mean and/or
variability.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_sens_trend(
  data,
  years,
  mean_trend = NULL,
  variability_trend = NULL,
  show_line = TRUE,
  ...
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{mean\_trend}] Trend in the mean estimated by by \code{\LinkA{eda\_sens\_trend()}{eda.Rul.sens.Rul.trend}}.

\item[\code{variability\_trend}] Trend in variability estimated by \code{\LinkA{eda\_sens\_trend()}{eda.Rul.sens.Rul.trend}}.

\item[\code{show\_line}] If \code{TRUE} (default), draw a fitted line through the data.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot containing:
\begin{itemize}

\item{} Gray points for each year’s annual maximum series value.
\item{} Optional gray line connecting the data if \code{show\_line = TRUE}.
\item{} A solid black line representing a constant mean, if \code{mean\_trend == NULL}.
\item{} A solid blue line representing a trend in the mean, if \code{mean\_trend != NULL}.
\item{} A dashed black line representing constant variability, if \code{variability\_trend == NULL}.
\item{} A dashed blue line representing a trend in variability, if \code{variability\_trend != NULL}.
\item{} The equation for the trend in the mean, written in the form \eqn{mx + b}{}.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{eda\_sens\_trend()}{eda.Rul.sens.Rul.trend}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
mean_trend <- eda_sens_trend(data, years)
plot_sens_trend(data, years, meant_trend = mean_trend)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_sffa}{Plot Return Levels and Confidence Intervals for S-FFA}{plot.Rul.sffa}
%
\begin{Description}
Generates a plot with return periods on the x-axis and return levels (annual
maxima magnitudes) on the y-axis for S-FFA. The confidence bound is shown as a
semi-transparent ribbon, and the point estimates are overlaid as a solid line.
Return periods are shown on a logarithmic scale.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_sffa(results, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A list of estimated return levels and confidence intervals
generated by \code{\LinkA{uncertainty\_bootstrap()}{uncertainty.Rul.bootstrap}} or \code{\LinkA{uncertainty\_rfpl()}{uncertainty.Rul.rfpl}}.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot showing:
\begin{itemize}

\item{} A semi-transparent gray ribbon between \code{results\$ci\_lower} and \code{results\$ci\_upper}.
\item{} A solid black line for the point estimates.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
results <- uncertainty_bootstrap(data, "WEI", "L-moments")
plot_sffa(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_spearman\_test}{Plot Spearman’s Rho Autocorrelation}{plot.Rul.spearman.Rul.test}
%
\begin{Description}
Visualizes Spearman’s rho serial correlation coefficients with shaded points
indicating statistical significance.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_spearman_test(results, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] A list generated by \code{\LinkA{eda\_spearman\_test()}{eda.Rul.spearman.Rul.test}}.

\item[\code{...}] Optional named arguments: 'title', 'xlabel', and 'ylabel'.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot showing:
\begin{itemize}

\item{} Vertical segments from \eqn{y=0}{} up to each \eqn{\rho}{} value at its lag.
\item{} Filled circles at each lag, filled black if serial correlation is detected.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
results <- eda_spearman_test(data)
plot_spearman_test(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{quantile\_fast}{Helper Function for Quantile Functions}{quantile.Rul.fast}
%
\begin{Description}
A helper function used by \code{\LinkA{quantile\_xxx()}{quantile.Rul.xxx}}.
This function does not validate parameters and is designed for use in other methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
quantile_fast(p, distribution, params, slice, structure)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{p}] Numeric vector of probabilities between 0 and 1 with no missing values.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.

\item[\code{slice}] Numeric scalar specifying the year at which to evaluate the
quantiles of a nonstationary probability distribution. The slice does not
have to be an element of the \code{years} argument. Note that if \code{structure\$location}
and \code{structure\$scale} are both \code{FALSE}, this argument will have no effect the
output of the function.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric vector of quantiles with the same length as \code{p}.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{quantile\_xxx()}{quantile.Rul.xxx}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize p, params, and structure
p <- runif(n = 10)
params <- c(0, 1, 1, 0)
structure <- list(location = FALSE, scale = TRUE)

# Compute the log-likelihood in the year 2000
quantile_fast(p, "GEV", params, 2000, structure)

\end{ExampleCode}
\end{Examples}
\HeaderA{quantile\_xxx}{Quantile Functions for Probability Models}{quantile.Rul.xxx}
\aliasA{quantile\_gev}{quantile\_xxx}{quantile.Rul.gev}
\aliasA{quantile\_glo}{quantile\_xxx}{quantile.Rul.glo}
\aliasA{quantile\_gno}{quantile\_xxx}{quantile.Rul.gno}
\aliasA{quantile\_gum}{quantile\_xxx}{quantile.Rul.gum}
\aliasA{quantile\_kap}{quantile\_xxx}{quantile.Rul.kap}
\aliasA{quantile\_lno}{quantile\_xxx}{quantile.Rul.lno}
\aliasA{quantile\_lp3}{quantile\_xxx}{quantile.Rul.lp3}
\aliasA{quantile\_nor}{quantile\_xxx}{quantile.Rul.nor}
\aliasA{quantile\_pe3}{quantile\_xxx}{quantile.Rul.pe3}
\aliasA{quantile\_wei}{quantile\_xxx}{quantile.Rul.wei}
%
\begin{Description}
Compute the quantiles for stationary and nonstationary variants of nine
different distributions: \code{"GUM"}, \code{"NOR"}, \code{"LNO"}, \code{"GEV"}, \code{"GLO"}, \code{"GNO"},
\code{"PE3"}, \code{"LP3"}, or \code{"WEI"}. In total, these methods compute the quantiles
for 36 different probability models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
quantile_gum(p, params, slice = 1900, structure = NULL)

quantile_nor(p, params, slice = 1900, structure = NULL)

quantile_lno(p, params, slice = 1900, structure = NULL)

quantile_gev(p, params, slice = 1900, structure = NULL)

quantile_glo(p, params, slice = 1900, structure = NULL)

quantile_gno(p, params, slice = 1900, structure = NULL)

quantile_pe3(p, params, slice = 1900, structure = NULL)

quantile_lp3(p, params, slice = 1900, structure = NULL)

quantile_wei(p, params, slice = 1900, structure = NULL)

quantile_kap(p, params, slice = 1900, structure = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{p}] Numeric vector of probabilities between 0 and 1 with no missing values.

\item[\code{params}] Numeric vector of distribution parameters, in the order (location,
scale, shape). The length must be between 2 and 5, depending on the specified
\code{distribution} and \code{structure}.

\item[\code{slice}] Numeric scalar specifying the year at which to evaluate the
quantiles of a nonstationary probability distribution. The slice does not
have to be an element of the \code{years} argument. Note that if \code{structure\$location}
and \code{structure\$scale} are both \code{FALSE}, this argument will have no effect the
output of the function.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric vector of quantiles with the same length as \code{p}.
\end{Value}
%
\begin{SeeAlso}
\code{\LinkA{quantile\_fast()}{quantile.Rul.fast}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize p and params
p <- runif(n = 10)
params <- c(0, 1, 0)

# Compute the quantiles
quantile_wei(p, params)

\end{ExampleCode}
\end{Examples}
\HeaderA{select\_ldistance}{L-Distance Method for Distribution Selection}{select.Rul.ldistance}
%
\begin{Description}
Selects a distribution from a set of candidate distributions by minimizing the
Euclidean distance between the theoretical L-moment ratios \eqn{(\tau_3, \tau_4)}{}
and the sample L-moment ratios \eqn{(t_3, t_4)}{}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
select_ldistance(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
For each candidate distribution, the method computes the Euclidean distance between
sample L-moment ratios (\eqn{\tau_3}{}, \eqn{\tau_4}{}) and the closest point on the
theoretical distribution's L-moment curve. For two-parameter distributions (Gumbel,
Normal, Log-Normal), the theoretical L-moment ratios are compared directly with
the sample L-moment ratios. The distribution with the minimum distance is selected.
If a distribution is fit to log-transformed data (Log-Normal or Log-Pearson Type
III), the L-moment ratios for the log-transformed sample are used instead.
\end{Details}
%
\begin{Value}
A list with the results of distribution selection:
\begin{itemize}

\item{} \code{method}: \code{"L-distance"}.
\item{} \code{data}: The \code{data} argument.
\item{} \code{metrics}: A list of L-distance metrics for each candidate distribution.
\item{} \code{recommendation}: The name of the distribution with the smallest L-distance.

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{lmom\_sample()}{lmom.Rul.sample}}, \code{\LinkA{select\_lkurtosis()}{select.Rul.lkurtosis}}, \code{\LinkA{select\_zstatistic()}{select.Rul.zstatistic}},
\code{\LinkA{plot\_lmom\_diagram()}{plot.Rul.lmom.Rul.diagram}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
select_ldistance(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{select\_lkurtosis}{L-Kurtosis Method for Distribution Selection}{select.Rul.lkurtosis}
%
\begin{Description}
Selects a probability distribution by minimizing the absolute distance
between the theoretical L-kurtosis (\eqn{\tau_4}{}) and the sample L-kurtosis
(\eqn{t_4}{}). Only supports 3-parameter distributions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
select_lkurtosis(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
This method computes the distance between the sample and theoretical L-kurtosis
values at a fixed L-skewness. For three parameter distributions, the shape parameter
that best replicates the sample L-skewness is determined using \code{\LinkA{stats::optim()}{stats::optim()}}.
\end{Details}
%
\begin{Value}
A list with the results of distribution selection:
\begin{itemize}

\item{} \code{method}: \code{"L-kurtosis"}.
\item{} \code{data}: The \code{data} argument.
\item{} \code{metrics}: A list of L-kurtosis metrics for each distribution.
\item{} \code{recommendation}: Name of the distribution with the smallest L-kurtosis metric.

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{lmom\_sample()}{lmom.Rul.sample}}, \code{\LinkA{select\_ldistance()}{select.Rul.ldistance}}, \code{\LinkA{select\_zstatistic()}{select.Rul.zstatistic}},
\code{\LinkA{plot\_lmom\_diagram()}{plot.Rul.lmom.Rul.diagram}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
select_lkurtosis(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{select\_zstatistic}{Z-Statistic Method for Distribution Selection}{select.Rul.zstatistic}
%
\begin{Description}
Selects the best-fit distribution by comparing a bias-corrected Z-statistic for
the sample L-kurtosis (\eqn{\tau_4}{}) against the theoretical L-moments for a set
of candidate distributions. The distribution with the smallest absolute Z-statistic
is selected.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
select_zstatistic(data, samples = 10000L)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{samples}] Integer scalar. The number of bootstrap samples. Default is 10000.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The method performs distribution selection using both raw and log-transformed data. The
distributions which use the raw data are GEV, GLO, PE3, GNO, and WEI. The LP3 distribution
uses the log-transformed data.

The Z-statistic is determined by fitting a four-parameter Kappa distribution to the
raw and log-transformed data. Then, bootstrapped samples from this Kappa distribution
The L-moments of these bootstrapped samples are used to estimate the Z-statistic
for each distribution.
\end{Details}
%
\begin{Value}
A list with the results of distribution selection:
\begin{itemize}

\item{} \code{method}: \code{"Z-selection"}.
\item{} \code{data}: The \code{data} argument.
\item{} \code{metrics}: List of computed Z-statistics for each candidate distribution.
\item{} \code{recommendation}: Name of the distribution with the smallest Z-statistic.
\item{} \code{reg\_params}: Kappa distribution parameters for the data.
\item{} \code{reg\_bias\_t4}: Bias of the L-kurtosis from the bootstrap.
\item{} \code{reg\_std\_t4}: Standard deviation of the L-kurtosis from the bootstrap.
\item{} \code{log\_params}: Kappa distribution parameters for the log-transformed data.
\item{} \code{log\_bias\_t4}: Bias of the L-kurtosis from the bootstrap (using \code{log\_params}).
\item{} \code{log\_std\_t4}: Standard deviation of the L-kurtosis from the bootstrap (using \code{log\_params}).

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{select\_ldistance()}{select.Rul.ldistance}}, \code{\LinkA{select\_lkurtosis()}{select.Rul.lkurtosis}}, \code{\LinkA{fit\_lmom\_kappa()}{fit.Rul.lmom.Rul.kappa}},
\code{\LinkA{quantile\_fast()}{quantile.Rul.fast}}, \code{\LinkA{plot\_lmom\_diagram()}{plot.Rul.lmom.Rul.diagram}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
select_zstatistic(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{uncertainty\_bootstrap}{Parametric Bootstrap Confidence Intervals for Flood Quantile Estimates}{uncertainty.Rul.bootstrap}
%
\begin{Description}
Computes return level estimates and confidence intervals at the specified return
periods (defaults to 2, 5, 10, 20, 50, and 100 years) using the parametric bootstrap
method. This function supports a variety of probability models and parameter estimation
methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
uncertainty_bootstrap(
  data,
  distribution,
  method,
  prior = NULL,
  years = NULL,
  structure = NULL,
  slices = 1900,
  alpha = 0.05,
  samples = 10000L,
  periods = c(2, 5, 10, 20, 50, 100)
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{method}] Character scalar specifying the estimation method.
Must be \code{"L-moments"}, \code{"MLE"}, or \code{"GMLE"}.

\item[\code{prior}] Numeric vector of length 2. Specifies the Beta prior shape
parameters \eqn{(p, q)}{} for the shape parameter \eqn{\kappa}{}.
Only used when \code{distribution = "GEV"}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}


\item[\code{slices}] Numeric vector specifying the years at which to evaluate the
return levels confidence intervals of a nonstationary probability distribution.
The slices do not have to be elements of the \code{years} argument.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{samples}] Integer scalar. The number of bootstrap samples. Default is 10000.

\item[\code{periods}] Numeric vector used to set the return periods for FFA.
All entries must be greater than or equal to 1.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The bootstrap procedure samples from the fitted distribution via inverse
transform sampling. For each bootstrapped sample, the parameters are re-estimated
using the \code{method} argument. Then, the bootstrapped parameters are used to compute
a new set of bootstrapped quantiles. Confidence intervals are obtained from the
empirical nonexceedance probabilities of the bootstrapped quantiles.
\end{Details}
%
\begin{Value}
A list containing the following three items:
\begin{itemize}

\item{} \code{method}: "Bootstrap"
\item{} \code{structure}: The value of the \code{structure} argument.
\item{} \code{slices}: A list of lists containing the results for each slice.

\end{itemize}


Each element of \code{slices} is a list with the following five items:
\begin{itemize}

\item{} \code{estimates}: Estimated quantiles for each return period.
\item{} \code{ci\_lower}: Lower bound of the confidence interval for each return period.
\item{} \code{ci\_upper}: Upper bound of the confidence interval for each return period.
\item{} \code{periods}: Vector of return periods; defaults to \code{c(2, 5, 10, 20, 50, 100)}.
\item{} \code{year}: The year at which the estimates were computed (nonstationary models only).

\end{itemize}

\end{Value}
%
\begin{Note}
The parametric bootstrap is known to give unreasonably wide confidence intervals
for small datasets. If this function detects a confidence interval that is 5+ times
wider than the return levels themselves, it will return an error and recommend
RFPL uncertainty quantification (with \code{uncertainty\_rfpl}).
\end{Note}
%
\begin{References}
Vidrio-Sahagún, C.T., He, J. Enhanced profile likelihood method for the nonstationary
hydrological frequency analysis, Advances in Water Resources 161, 10451 (2022).
\Rhref{https://doi.org/10.1016/j.advwatres.2022.104151}{doi:10.1016\slash{}j.advwatres.2022.104151}
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{fit\_lmom\_fast()}{fit.Rul.lmom.Rul.fast}}, \code{\LinkA{fit\_maximum\_likelihood()}{fit.Rul.maximum.Rul.likelihood}}, \code{\LinkA{lmom\_sample()}{lmom.Rul.sample}},
\code{\LinkA{quantile\_fast()}{quantile.Rul.fast}}, \code{\LinkA{plot\_sffa()}{plot.Rul.sffa}}, \code{\LinkA{plot\_nsffa()}{plot.Rul.nsffa}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
uncertainty_bootstrap(data, "WEI", "L-moments")

\end{ExampleCode}
\end{Examples}
\HeaderA{uncertainty\_rfpl}{(Generalized) Regula-Falsi Confidence Intervals for Flood Quantile Estimates}{uncertainty.Rul.rfpl}
%
\begin{Description}
Calculates return level estimates and confidence intervals at specified return
periods (defaults to 2, 5, 10, 20, 50, and 100 years) using the Regula-Falsi profile
likelihood or Regula-Falsi generalized profile likelihood root‐finding methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
uncertainty_rfpl(
  data,
  distribution,
  prior = NULL,
  years = NULL,
  structure = NULL,
  slices = 1900,
  alpha = 0.05,
  eps = 0.01,
  periods = c(2, 5, 10, 20, 50, 100)
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric vector of annual maximum series values.
Must be strictly positive, finite, and not missing.

\item[\code{distribution}] Character scalar. A three-character code indicating
a distribution family. Must be one of: \code{"GUM"}, \code{"NOR"}, \code{"LNO"},
\code{"GEV"}, \code{"GLO"}, \code{"GNO"}, \code{"PE3"}, \code{"LP3"}, or \code{"WEI"}.

\item[\code{prior}] Numeric vector of length 2. Specifies the Beta prior shape
parameters \eqn{(p, q)}{} for the shape parameter \eqn{\kappa}{}.
Only used when \code{distribution = "GEV"}.

\item[\code{years}] Numeric vector of observation years corresponding to \code{data}.
Must be the same length as \code{data} and strictly increasing.

\item[\code{structure}] Named list indicating which distribution parameters are
modeled as nonstationary. Must contain two logical scalars:
\begin{itemize}

\item{} \code{location}: If \code{TRUE}, the location parameter has a linear temporal trend.
\item{} \code{scale}: If \code{TRUE}, the scale parameter has a linear temporal trend.

\end{itemize}


\item[\code{slices}] Numeric vector specifying the years at which to evaluate the
return levels confidence intervals of a nonstationary probability distribution.
The slices do not have to be elements of the \code{years} argument.

\item[\code{alpha}] Numeric scalar in \eqn{[0.01, 0.1]}{}. The significance
level for confidence intervals or hypothesis tests. Default is 0.05.

\item[\code{eps}] Numeric scalar. The log-likelihood tolerance for the Regula-Falsi
convergence (default is 0.01).

\item[\code{periods}] Numeric vector used to set the return periods for FFA.
All entries must be greater than or equal to 1.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
\begin{enumerate}

\item{} Fits the distribution using \code{\LinkA{fit\_maximum\_likelihood()}{fit.Rul.maximum.Rul.likelihood}} to obtain parameter
estimates and log‐likelihood.
\item{} Defines an objective function \eqn{f(y_p, p)}{} using the reparameterized
log-likelihood function.
\item{} Iteratively brackets the root by rescaling initial guesses by 0.05 until
\eqn{f(y_p, p)}{} changes sign.
\item{} Uses the Regula Falsi method to solve \eqn{f(y_p, p) = 0}{} for each
return-period probability.
\item{} Returns lower and upper confidence bounds at significance level \code{alpha}
given the return level estimates.

\end{enumerate}

\end{Details}
%
\begin{Value}
A list containing the following three items:
\begin{itemize}

\item{} \code{method}: "Bootstrap"
\item{} \code{structure}: The value of the \code{structure} argument.
\item{} \code{slices}: A list of lists containing the results for each slice.

\end{itemize}


Each element of \code{slices} is a list with the following five items:
\begin{itemize}

\item{} \code{estimates}: Estimated quantiles for each return period.
\item{} \code{ci\_lower}: Lower bound of the confidence interval for each return period.
\item{} \code{ci\_upper}: Upper bound of the confidence interval for each return period.
\item{} \code{periods}: Vector of return periods; defaults to \code{c(2, 5, 10, 20, 50, 100)}.
\item{} \code{year}: The year at which the estimates were computed (nonstationary models only).

\end{itemize}

\end{Value}
%
\begin{Note}
Although the more modern \code{\LinkA{stats::optim()}{stats::optim()}} function is preferred over
\code{\LinkA{stats::nlminb()}{stats::nlminb()}}, we use \code{\LinkA{stats::nlminb()}{stats::nlminb()}} because it supports infinite
values of the likelihood function.

RFPL uncertainty quantification can be numerically unstable for some datasets.
If this function encounters an issue, it will return an error and recommend
using the parametric bootstrap method (\code{uncertainty\_bootstrap}) instead.
\end{Note}
%
\begin{References}
Vidrio-Sahagún, C.T., He, J. Enhanced profile likelihood method for the nonstationary
hydrological frequency analysis, Advances in Water Resources 161, 10451 (2022).
\Rhref{https://doi.org/10.1016/j.advwatres.2022.104151}{doi:10.1016\slash{}j.advwatres.2022.104151}

Vidrio-Sahagún, C.T., He, J. \& Pietroniro, A. Multi-distribution regula-falsi profile
likelihood method for nonstationary hydrological frequency analysis. Stoch Environ Res
Risk Assess 38, 843–867 (2024). \Rhref{https://doi.org/10.1007/s00477-023-02603-0}{doi:10.1007\slash{}s00477\-023\-02603\-0}
\end{References}
%
\begin{SeeAlso}
\code{\LinkA{quantile\_fast()}{quantile.Rul.fast}}, \code{\LinkA{uncertainty\_bootstrap()}{uncertainty.Rul.bootstrap}},
\code{\LinkA{plot\_sffa()}{plot.Rul.sffa}}, \code{\LinkA{plot\_nsffa()}{plot.Rul.nsffa}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
uncertainty_rfpl(data, "GLO")

\end{ExampleCode}
\end{Examples}
\printindex{}
\end{document}
