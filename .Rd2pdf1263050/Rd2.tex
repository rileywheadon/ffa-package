\nonstopmode{}
\documentclass[a4paper]{book}
\usepackage[times,hyper]{Rd}
\usepackage{makeidx}
\makeatletter\@ifl@t@r\fmtversion{2018/04/01}{}{\usepackage[utf8]{inputenc}}\makeatother
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `ffaframework'}}
\par\bigskip{\large \today}
\end{center}
\ifthenelse{\boolean{Rd@use@hyper}}{\hypersetup{pdftitle = {ffaframework: Flood Frequency Analysis Framework}}}{}
\ifthenelse{\boolean{Rd@use@hyper}}{\hypersetup{pdfauthor = {Riley Wheadon}}}{}
\begin{description}
\raggedright{}
\item[Title]\AsIs{Flood Frequency Analysis Framework}
\item[Version]\AsIs{0.1.0}
\item[Author]\AsIs{Riley Wheadon [aut, cre]}
\item[Maintainer]\AsIs{Riley Wheadon }\email{rileywheadon@gmail.com}\AsIs{}
\item[Description]\AsIs{Tools for exploratory data analysis and flood frequency analysis. Implements statistical tests and methods for distribution selection, parameter estimation, uncertainty quantification, and model assessment.}
\item[License]\AsIs{CC BY 4.0}
\item[Encoding]\AsIs{UTF-8}
\item[LazyData]\AsIs{true}
\item[Depends]\AsIs{R (>= 4.5.0), ggplot2, patchwork}
\item[Suggests]\AsIs{testthat, vdiffr}
\item[Roxygen]\AsIs{list(markdown = TRUE)}
\item[RoxygenNote]\AsIs{7.3.2}
\end{description}
\Rdcontents{Contents}
\HeaderA{ams.decomposition}{Decompose Annual Maximum Streamflow Data}{ams.decomposition}
%
\begin{Description}
Removes trends in the means and/or variances of annual maximum streamflow (AMS)
data using Sen’s trend estimator and a moving‐window estimator of the variance.
Four scenarios are supported:
\begin{enumerate}

\item{} No trend. The data is returned unmodified.
\item{} Trend in means only.
\item{} Trend in variance only.
\item{} Trends in both means and variance.

\end{enumerate}

\end{Description}
%
\begin{Usage}
\begin{verbatim}
ams.decomposition(data, years, signature)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{signature}] Character (1); Indicates trend(s) to remove.  Must be one of:
\begin{itemize}

\item{} \code{NULL}: Do not remove any trends.
\item{} \code{"10"}: Remove linear trend in the mean.
\item{} \code{"01"}: Remove linear trend in the variance.
\item{} \code{"11"}: Remove trends in both the mean and variance.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Details}
Internally, the function does the following:
\begin{enumerate}

\item{} If \code{signature == NULL}, returns \code{data} without modification.
\item{} Constructs a \code{covariate} based on \code{years} using the formula \code{(years - 1900) / 100}.
\item{} If \code{signature == "10"}, fits Sen’s trend estimator to \code{data} and \code{covariate}
and removes the fitted linear mean trend.
\item{} If \code{signature == "01"}, computes moving‐window variances using \LinkA{mw.variance}{mw.variance},
fits Sen’s trend estimator to those variances, and rescales the series to remove
trends in the variance.
\item{} If \code{signature == "11"}, applies (3) then (4) sequentially.
\item{} Ensures all returned values are greater than 1 by shifting the decomposed data.

\end{enumerate}

\end{Details}
%
\begin{Value}
Numeric; a vector with the same length as \code{data} and \code{years} containing
the decomposed AMS data with the specified trend(s) removed.
\end{Value}
%
\begin{SeeAlso}
\LinkA{mw.variance}{mw.variance}, \LinkA{sens.trend}{sens.trend}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
ams.decomposition(data, years, "10")

\end{ExampleCode}
\end{Examples}
\HeaderA{assessment.plot}{Plot Model Assessment Results}{assessment.plot}
%
\begin{Description}
Creates a quantile–quantile plot comparing observed annual maximum streamflow (AMS)
data to quantile estimates from a fitted parametric model. The 1:1 line is drawn in
black and the model estimates are plotted as semi‐transparent red points.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
assessment.plot(data, assessment)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{assessment}] List; model assessment results generated by \LinkA{model.assessment}{model.assessment}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot containing:
\begin{itemize}

\item{} A black line for the theoretical 1:1 relationship between observed and model quantiles.
\item{} Red points marking the estimated quantiles against the observed quantiles.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize example data, years, and params
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
params <- c(100, 10)

# Perform uncertainty analysis
uncertainty <- sb.uncertainty(data, years, "NOR", "L-moments")

# Perform model assessment
assessment <- model.assessment(data, years, "NOR", params, uncertainty)

# Generate a model assessment plot
assessment.plot(data, assessment)

\end{ExampleCode}
\end{Examples}
\HeaderA{bbmk.plot}{Plot Block‐Bootstrap Mann–Kendall Test Results}{bbmk.plot}
%
\begin{Description}
Generates a histogram of bootstrapped Mann–Kendall S‐statistics with vertical
lines indicating the observed S‐statistic and confidence bounds.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
bbmk.plot(results)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] List; BB‐MK test results generated by \LinkA{bbmk.test}{bbmk.test}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; A plot containing:
\begin{itemize}

\item{} A gray histogram of the distribution of bootstrapped S‐statistics.
\item{} A red vertical line at the lower and upper confidence bounds.
\item{} A black vertical line at the observed S‐statistic.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
results <- bbmk.test(data, n_sim = 1000)
bbmk.plot(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{bbmk.test}{Block-Bootstrap Mann-Kendall Test for Trend Detection}{bbmk.test}
%
\begin{Description}
Performs a bootstrapped version of the Mann-Kendall trend test to account
for serial correlation in annual maximum streamflow (AMS) data. The procedure
uses Spearman’s autocorrelation test to estimate the least insignificant lag,
then applies a bootstrap procedure to obtain the empirical p-value and confidence
bounds for the Mann-Kendall S-statistic.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
bbmk.test(data, alpha = 0.05, n_sim = 10000, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{n\_sim}] Integer (1); the number of bootstrap simulations (default is 10000).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The block size for the bootstrap is selected as \code{least\_lag + 1}, where \code{least\_lag}
is estimated using \LinkA{spearman.test}{spearman.test}. Each bootstrap sample is generated by
resampling blocks of the original data  (without replacement) and computing the
Mann-Kendall S-statistic. This procedure adjusts for autocorrelation in the data.
\end{Details}
%
\begin{Value}
List; the results of the test, including:
\begin{itemize}

\item{} \code{s.bootstrap}: Vector of bootstrapped test statistics used for plotting.
\item{} \code{s.statistic}: The Mann-Kendall test statistic computed on the original series.
\item{} \code{p.value}: Empirical two-sided p-value computed from the bootstrap distribution.
\item{} \code{bounds}: Confidence interval bounds for the null distribution of the statistic.
\item{} \code{reject}: Logical. TRUE if the null hypothesis was rejected at significance level alpha.
\item{} \code{msg}: Character string summarizing the test result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{mk.test}{mk.test}, \LinkA{spearman.test}{spearman.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
bbmk.test(data, n_sim = 1000)

\end{ExampleCode}
\end{Examples}
\HeaderA{gll-functions}{Generalized Log-Likelihood Functions for GEV Models}{gll.Rdash.functions}
\aliasA{gllgev}{gll-functions}{gllgev}
\aliasA{gllgev100}{gll-functions}{gllgev100}
\aliasA{gllgev110}{gll-functions}{gllgev110}
%
\begin{Description}
Computes the generalized log-likelihood for stationary and non-stationary variants of the
Generalized Extreme Value (GEV) distribution with a Beta prior on the shape parameter.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
gllgev(data, params, prior, years = NULL)

gllgev100(data, params, prior, years)

gllgev110(data, params, prior, years)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{params}] Numeric; a vector of parameters. Must have the correct length for the model.

\item[\code{prior}] Numeric (2); a vector of parameters\eqn{(p, q)}{} of the Beta prior on \eqn{\kappa}{}.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.
Only required for \code{gllgev100()} and \code{gllgev110()}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The generalized log-likelihood is defined as sum of the log-likelihood of the specified
model and the log-density of the Beta prior with parameters \eqn{(p, q)}{}. The contribution
of the prior is: \deqn{\log \pi(\kappa) = (p-1) \log(0.5-\kappa) + (q-1) \log(0.5+\kappa) 
- \log (\beta(p, q))}{}

Each function corresponds to a different parameterization of the GEV model:
\begin{itemize}

\item{} \code{gllgev()}: Stationary location and scale, 3 parameters.
\item{} \code{gllgev100()}: Time-varying location, stationary scale, 4 parameters.
\item{} \code{gllgev110()}: Time-varying location and scale, 5 parameters.

\end{itemize}

\end{Details}
%
\begin{Value}
Numeric (1); the generalized log-likelihood value.
\end{Value}
%
\begin{Note}
The \code{gllgev}, \code{gllgev100}, and \code{gllgev110} functions perform extensive parameter validation,
which can be slow. If you plan to call these methods often, it is recommended to use
the \code{gllxxx} helper function instead.
\end{Note}
%
\begin{SeeAlso}
\LinkA{gllxxx}{gllxxx}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data, years, params, and prior
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
params <- c(0, 1, 1, 0)
prior <- c(5, 10)

# Compute the generalized log-likelihood
gllgev100(data, params, prior, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{gllxxx}{Generalized Log-Likelihood Helper Function}{gllxxx}
%
\begin{Description}
A helper function used by \LinkA{gllgev}{gllgev}, \LinkA{gllgev100}{gllgev100}, and \LinkA{gllgev110}{gllgev110}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
gllxxx(name, signature, data, params, prior, covariate = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{name}] Character (1); the name of the probability distribution.

\item[\code{signature}] Character (1); the non-stationary signature (\code{NULL}, \code{"10"}, or \code{"11"}).

\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{params}] Numeric; a vector of parameters. Must have the correct length for the model.

\item[\code{prior}] Numeric (2); a vector of parameters \eqn{(p, q)}{} of the Beta prior
on \eqn{\kappa}{}.

\item[\code{covariate}] Numeric; a vector with the same length as \code{data}.
Required if \code{signature} is \code{"10"} or \code{"11"}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Numeric (1); the generalized log-likelihood value.
\end{Value}
%
\begin{Note}
This function does not perform parameter validation, which improves performance
but may cause unpredictable behaviour. Use at your own risk.
\end{Note}
%
\begin{SeeAlso}
\LinkA{gll-functions}{gll.Rdash.functions}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data, params, params, prior, and covariate
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(0, 1, 1, 0, 0)
prior <- c(5, 10)
covariate <- seq(0, 1, length.out = 100)

# Compute the generalized log-likelihood
gllxxx("GEV", "11", data, params, prior, covariate)

\end{ExampleCode}
\end{Examples}
\HeaderA{kpss.test}{Kwiatkowski–Phillips–Schmidt–Shin (KPSS) Unit Root Test}{kpss.test}
%
\begin{Description}
Performs the KPSS unit root test on annual maximum streamflow (AMS) data.
The null hypothesis is that  the time series is trend-stationary with a linear
trend and constant drift.  The alternative hypothesis is that the time series
has a unit root and is non-stationary.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kpss.test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The implementation of the KPSS test is based on the \pkg{aTSA} package, which interpolates
a significance table from Hobjin et al. (2004). Therefore, a result of \eqn{p = 0.01}{}
implies that \eqn{p \leq 0.01}{} and a result of \eqn{p = 0.10}{} implies that \eqn{p \geq 0.10}{}.
This implementation uses the Type III KPSS test, which accounts for a linear trend in the data.
\end{Details}
%
\begin{Value}
List; the test results, consisting of:
\begin{itemize}

\item{} \code{statistic}: The KPSS test statistic.
\item{} \code{p.value}: The reported p-value from the test. See notes regarding discrete thresholds.
\item{} \code{reject}: Logical. TRUE if the null hypothesis is rejected at \code{alpha}.
\item{} \code{msg}: Character string summarizing the test outcome, printed if \code{quiet = FALSE}.

\end{itemize}

\end{Value}
%
\begin{References}
Hobijn, B., Franses, P.H. and Ooms, M. (2004), Generalizations of the KPSS-test for
stationarity. Statistica Neerlandica, 58: 483-502.
\end{References}
%
\begin{SeeAlso}
\LinkA{pp.test}{pp.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
kpss.test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{ld.selection}{L-Distance Method for Distribution Selection}{ld.selection}
%
\begin{Description}
Selects a distribution from a set of candidate distributions by minimizing the
Euclidean distance between the theoretical L-moment ratios \eqn{(\tau_3, \tau_4)}{}
and the sample L-moment ratios \eqn{(t_3, t_4)}{}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ld.selection(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
For each candidate distribution, the method computes the Euclidean distance between
sample L-moment ratios (\eqn{\tau_3}{}, \eqn{\tau_4}{}) and the closest point on the
theoretical distribution's L-moment curve. For two-parameter distributions (Gumbel,
Normal, Log-Normal), the theoretical L-moment ratios are compared directly with
the sample L-moment ratios. The distribution with the minimum distance is selected.
If a distribution is fit to log-transformed data (Log-Normal or Log-Pearson Type III),
the L-moment ratios for the log-transformed sample are used instead.
\end{Details}
%
\begin{Value}
List; results of distribution selection:
\begin{itemize}

\item{} \code{method}: \code{"L-distance"}
\item{} \code{metrics}: A list of L-distance metrics for each candidate distribution.
\item{} \code{recommendation}: The name of the distribution with the smallest L-distance.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{lmom.sample}{lmom.sample}, \LinkA{lk.selection}{lk.selection}, \LinkA{z.selection}{z.selection}, \LinkA{optim}{optim}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
ld.selection(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{lk.selection}{L-Kurtosis Method for Distribution Selection}{lk.selection}
%
\begin{Description}
Selects a probability distribution by minimizing the absolute distance
between the theoretical L-kurtosis (\eqn{\tau_4}{}) and the sample L-kurtosis
(\eqn{t_4}{}). For 3-parameter distributions, we use the shape parameter that
best replicates the sample L-skewness (\eqn{t_3}{}) of the data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lk.selection(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
This method computes the distance between the sample and theoretical L-kurtosis values at
a fixed L-skewness. For three parameter distributions, the shape parameter that best
replicates the sample L-skewness is derived using \LinkA{optim}{optim}.
\end{Details}
%
\begin{Value}
List; results of distribution selection:
\begin{itemize}

\item{} \code{method}: \code{"L-kurtosis"}
\item{} \code{metrics}: A list of L-kurtosis metrics for each distribution.
\item{} \code{recommendation}: Name of the distribution with the smallest L-kurtosis metric

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{lmom.sample}{lmom.sample}, \LinkA{ld.selection}{ld.selection}, \LinkA{z.selection}{z.selection}, \LinkA{optim}{optim}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
lk.selection(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{llv-functions}{Log-Likelihood Functions for Probability Models}{llv.Rdash.functions}
\aliasA{llvgev}{llv-functions}{llvgev}
\aliasA{llvgev100}{llv-functions}{llvgev100}
\aliasA{llvgev110}{llv-functions}{llvgev110}
\aliasA{llvglo}{llv-functions}{llvglo}
\aliasA{llvglo100}{llv-functions}{llvglo100}
\aliasA{llvglo110}{llv-functions}{llvglo110}
\aliasA{llvgno}{llv-functions}{llvgno}
\aliasA{llvgno100}{llv-functions}{llvgno100}
\aliasA{llvgno110}{llv-functions}{llvgno110}
\aliasA{llvgum}{llv-functions}{llvgum}
\aliasA{llvgum10}{llv-functions}{llvgum10}
\aliasA{llvgum11}{llv-functions}{llvgum11}
\aliasA{llvlno}{llv-functions}{llvlno}
\aliasA{llvlno10}{llv-functions}{llvlno10}
\aliasA{llvlno11}{llv-functions}{llvlno11}
\aliasA{llvlp3}{llv-functions}{llvlp3}
\aliasA{llvlp3100}{llv-functions}{llvlp3100}
\aliasA{llvlp3110}{llv-functions}{llvlp3110}
\aliasA{llvnor}{llv-functions}{llvnor}
\aliasA{llvnor10}{llv-functions}{llvnor10}
\aliasA{llvnor11}{llv-functions}{llvnor11}
\aliasA{llvpe3}{llv-functions}{llvpe3}
\aliasA{llvpe3100}{llv-functions}{llvpe3100}
\aliasA{llvpe3110}{llv-functions}{llvpe3110}
\aliasA{llvwei}{llv-functions}{llvwei}
\aliasA{llvwei100}{llv-functions}{llvwei100}
\aliasA{llvwei110}{llv-functions}{llvwei110}
%
\begin{Description}
Compute the log-likelihood value for stationary and non-stationary variants
of nine different distributions (\code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO},
\code{PE3}, \code{LP3}, and \code{WEI}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
llvgum(data, params, years = NULL)

llvgum10(data, params, years)

llvgum11(data, params, years)

llvnor(data, params, years = NULL)

llvnor10(data, params, years)

llvnor11(data, params, years)

llvlno(data, params, years = NULL)

llvlno10(data, params, years)

llvlno11(data, params, years)

llvgev(data, params, years = NULL)

llvgev100(data, params, years)

llvgev110(data, params, years)

llvglo(data, params, years = NULL)

llvglo100(data, params, years)

llvglo110(data, params, years)

llvgno(data, params, years = NULL)

llvgno100(data, params, years)

llvgno110(data, params, years)

llvpe3(data, params, years = NULL)

llvpe3100(data, params, years)

llvpe3110(data, params, years)

llvlp3(data, params, years = NULL)

llvlp3100(data, params, years)

llvlp3110(data, params, years)

llvwei(data, params, years = NULL)

llvwei100(data, params, years)

llvwei110(data, params, years)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{params}] Numeric; a vector of parameters. Must have the correct length for the model.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.
Required for non-stationary models, which end in \code{10}, \code{11}, \code{100}, or \code{110}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The log-likelihood is the logarithm of the probability density function.
For two-parameter distributions (\code{GUM}, \code{NOR}, \code{LNO}), there are three
different \code{llv} functions:
\begin{itemize}

\item{} \code{llv...()}: Stationary location and scale, 2 parameters.
\item{} \code{llv...10()}: Time-varying location, stationary scale, 3 parameters.
\item{} \code{llv...11()}: Time-varying location and scale, 4 parameters.

\end{itemize}


For three-parameter distributions (\code{GEV}, \code{GLO}, \code{GNO}, \code{PE3}, \code{LP3}, \code{WEI}),
there are also three different \code{llv} functions:
\begin{itemize}

\item{} \code{llv...()}: Stationary location and scale, 3 parameters.
\item{} \code{llv...100()}: Time-varying location, stationary scale, 4 parameters.
\item{} \code{llv...110()}: Time-varying location and scale, 5 parameters.

\end{itemize}

\end{Details}
%
\begin{Value}
Numeric (1); the log-likelihood value.
\end{Value}
%
\begin{Note}
The \code{llv...} functions perform extensive parameter validation, which can be slow.
If you plan to make calls these methods often, it is recommended to use the \LinkA{llvxxx}{llvxxx}
helper function instead.
\end{Note}
%
\begin{SeeAlso}
\LinkA{llvxxx}{llvxxx}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data, years, and params
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
params <- c(0, 1, 1, 0)

# Compute the log-likelihood
llvgno100(data, params, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{llvxxx}{Log-Likelihood Helper Function}{llvxxx}
%
\begin{Description}
A helper function used by \LinkA{llv-functions}{llv.Rdash.functions}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
llvxxx(name, signature, data, params, covariate = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{name}] Character (1); the name of the probability distribution.

\item[\code{signature}] Character (1); the non-stationary signature (\code{NULL}, \code{"10"}, or \code{"11"}).

\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{params}] Numeric; a vector of parameters. Must have the correct length for the model.

\item[\code{covariate}] Numeric; a vector with the same length as \code{data}.
Required if \code{signature} is \code{"10"} or \code{"11"}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Numeric (1); the log-likelihood value.
\end{Value}
%
\begin{Note}
This function does not perform parameter validation, which improves performance
at the cost of potentially unpredictable behaviour. Use at your own risk.
\end{Note}
%
\begin{SeeAlso}
\LinkA{llv-functions}{llv.Rdash.functions}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data and params
data <- rnorm(n = 100, mean = 100, sd = 10)
params <- c(0, 1, 0)

# Compute the log-likelihood
llvxxx("GEV", NULL, data, params)

\end{ExampleCode}
\end{Examples}
\HeaderA{lmom.plot}{Plot L-Moment Ratio Diagram}{lmom.plot}
%
\begin{Description}
Generates a plot of L-moment ratios with the L-skewness on the x-axis and L-kurtosis
on the y-axis. Plots the sample and log-sample L-moment ratios alongside the theoretical
L-moment ratios for a set of candidate distributions. If the selection method is
\LinkA{ld.selection}{ld.selection} or \LinkA{lk.selection}{lk.selection}, the plot will include a small inset around
the L-moment ratios of the recommended distribution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmom.plot(data, results)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{results}] List; output of \LinkA{ld.selection}{ld.selection}, \LinkA{lk.selection}{lk.selection},
or \LinkA{z.selection}{z.selection}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; plot object containing the L-moment ratio diagram, with:
\begin{itemize}

\item{} L-moment ratio curves for each 3-parameter distribution.
\item{} Points for the L-moment ratios of each 2-parameter distribution.
\item{} Sample and log-sample L-moment ratio \eqn{(t_3, t_4)}{} points.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{ld.selection}{ld.selection}, \LinkA{lk.selection}{lk.selection}, \LinkA{z.selection}{z.selection}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
results <- ld.selection(data)
lmom.plot(data, results)

\end{ExampleCode}
\end{Examples}
\HeaderA{lmom.sample}{Sample L-moments}{lmom.sample}
%
\begin{Description}
Computes the first four sample L-moments and L-moment ratios from a numeric
vector of data. L-moments are linear combinations of order statistics that
provide robust alternatives to conventional moments, with advantages in
parameter estimation for heavy-tailed and skewed distributions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmom.sample(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Given probability weighted moments \eqn{\beta_0, \beta_1, \beta_2, \beta_3}{}, the
first four sample L-moments are:
\begin{itemize}

\item{} \eqn{l_1 = \beta_0}{}
\item{} \eqn{l_2 = 2\beta_1 - \beta_0}{}
\item{} \eqn{l_3 = 6\beta_2 - 6\beta_1 + \beta_0}{}
\item{} \eqn{l_4 = 20\beta_3 - 30\beta_2 + 12\beta_1 - \beta_0}{}

\end{itemize}


Then, the sample L-skewness is \eqn{t_3 = l_3 / l_2}{} and the sample L-kurtosis
is \eqn{t_4 = l_4 / l_2}{}.
\end{Details}
%
\begin{Value}
Numeric (4); a vector containing the first four L-moments and L-moment ratios:
\begin{itemize}

\item{} \code{l1}: L-mean
\item{} \code{l2}: L-variance
\item{} \code{t3}: L-skewness
\item{} \code{t4}: L-kurtosis

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J. R. M. (1990). L-moments: Analysis and estimation of distributions
using linear combinations of order statistics. \emph{Journal of the Royal Statistical
Society: Series B (Methodological)}, 52(1), 105–124.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
lmom.sample(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{lmr-functions}{Theoretical L-moments of Probability Distributions}{lmr.Rdash.functions}
\aliasA{lmrgev}{lmr-functions}{lmrgev}
\aliasA{lmrglo}{lmr-functions}{lmrglo}
\aliasA{lmrgno}{lmr-functions}{lmrgno}
\aliasA{lmrgum}{lmr-functions}{lmrgum}
\aliasA{lmrnor}{lmr-functions}{lmrnor}
\aliasA{lmrpe3}{lmr-functions}{lmrpe3}
\aliasA{lmrwei}{lmr-functions}{lmrwei}
%
\begin{Description}
Computes the first four L-moments and L-moment ratios of seven different
probability distributions (\code{GUM}, \code{NOR}, \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3}, and \code{WEI})
given the parameters of the distribution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmrgum(params)

lmrnor(params)

lmrgev(params)

lmrglo(params)

lmrgno(params)

lmrpe3(params)

lmrwei(params)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{params}] Numeric; a vector of parameters. Must have the correct length for the model.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The distributions \code{GUM}, \code{NOR}, \code{GEV}, \code{GLO}, and \code{WEI} have closed-form solutions
for the L-moments and L-moment ratios in terms of the parameters. The distributions
\code{GNO} and \code{PE3} use rational approximations of the L-moment ratios from Hosking (1997).
\end{Details}
%
\begin{Value}
A numeric vector of length 4 containing:
\begin{itemize}

\item{} \eqn{\lambda_1}{}: L-mean
\item{} \eqn{\lambda_2}{}: L-variance
\item{} \eqn{\tau_3}{}: L-skewness
\item{} \eqn{\tau_4}{}: L-kurtosis

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
lmrgev(c(0, 1, 0))

\end{ExampleCode}
\end{Examples}
\HeaderA{lmrxxx}{Helper Function for L-moments Ratios}{lmrxxx}
%
\begin{Description}
A helper function used by \LinkA{lmr-functions}{lmr.Rdash.functions}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lmrxxx(model, params)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] Character; \code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO},
\code{PE3}, \code{LP3}, or \code{WEI}.

\item[\code{params}] Numeric; a vector of parameters.
\begin{itemize}

\item{} Numeric (2) if \code{model} is \code{GUM}, \code{NOR}, or \code{LNO}.
\item{} Numeric (3) if \code{model} is \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3}, \code{LP3}, or \code{WEI}.

\end{itemize}

\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric vector of length 4 containing:
\begin{itemize}

\item{} \eqn{\lambda_1}{}: L-mean
\item{} \eqn{\lambda_2}{}: L-variance
\item{} \eqn{\tau_3}{}: L-skewness
\item{} \eqn{\tau_4}{}: L-kurtosis

\end{itemize}

\end{Value}
%
\begin{Note}
L-moment ratios for \code{NOR}/\code{LNO} and \code{PE3}/\code{LP3} are identical
since it is assumed that the \code{LNO}/\code{LP3} L-moments will be compared
to the sample L-moments of the logarithm of the data.
\end{Note}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
lmrxxx("GLO", c(0, 1, 0))

\end{ExampleCode}
\end{Examples}
\HeaderA{mk.test}{Mann–Kendall Trend Test}{mk.test}
%
\begin{Description}
Performs the Mann–Kendall trend test on a numeric vector to detect the presence
of a monotonic trend (increasing or decreasing) over time. The test is non-parametric
and accounts for tied observations in the data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mk.test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The statistic \eqn{S}{} is computed as the sum over all pairs \eqn{i < j}{} of the sign of the
difference \eqn{x_j - x_i}{}. Ties are explicitly accounted for when calculating the variance of
\eqn{S}{}, using grouped frequencies of tied observations. The test statistic \eqn{Z}{} is then
computed based on the sign and magnitude of \eqn{S}{}, and the p-value is derived from the
standard normal distribution.
\end{Details}
%
\begin{Value}
List; the test results:
\begin{itemize}

\item{} \code{s.statistic}: The raw Mann–Kendall test statistic \eqn{S}{}.
\item{} \code{s.variance}: The variance of the test statistic under the null hypothesis.
\item{} \code{p.value}: The p-value associated with the two-sided hypothesis test.
\item{} \code{reject}: Logical. TRUE if the null hypothesis of no trend is rejected at \code{alpha}.
\item{} \code{msg}: A character string summarizing the result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{bbmk.test}{bbmk.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
mk.test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{mks.plot}{Plot Mann–Kendall–Sneyers (MKS) Test Results}{mks.plot}
%
\begin{Description}
Constructs a two‐panel visualization of the MKS test. The upper panel plots the
normalized progressive and regressive Mann–Kendall S‐statistics over time, with
dashed confidence bounds and potential trend‐change points. The lower panel
contains the annual maximum streamflow (AMS) data with the change points highlighted
points along with an optional trend line.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mks.plot(data, years, results, show_trend = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{results}] List; results of the MKS test, generated by \LinkA{mks.test}{mks.test}.

\item[\code{show\_trend}] Logical; if TRUE (default), draw a fitted line through the AMS data.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{patchwork}; A plot object with two \code{ggplot2} panels stacked vertically.
\end{Value}
%
\begin{SeeAlso}
\LinkA{mks.test}{mks.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
results <- mks.test(data, years)
mks.plot(data, years, results)

\end{ExampleCode}
\end{Examples}
\HeaderA{mks.test}{Mann–Kendall–Sneyers Test for Change Point Detection}{mks.test}
%
\begin{Description}
Performs the Mann–Kendall–Sneyers (MKS) test to detect the beginning of a monotonic
trend in annual maximum streamflow (AMS) data. The test computes normalized
progressive and regressive Mann–Kendall statistics and identifies statistically
significant crossing points, indicating potential change points in the trend.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mks.test(data, years, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function computes progressive and regressive Mann–Kendall statistics \eqn{S_t}{},
normalized by their expected values and variances under the null hypothesis. The crossing
points where the difference between these normalized statistics changes sign are
identified using linear interpolation. The significance of detected crossings is
assessed using normal quantiles and the maximum absolute crossing statistic.
\end{Details}
%
\begin{Value}
List; results of the MKS test:
\begin{itemize}

\item{} \code{s.progressive}: Normalized progressive Mann–Kendall statistics.
\item{} \code{s.regressive}: Normalized regressive Mann–Kendall statistics.
\item{} \code{bound}: Critical confidence bound for significance based on \code{alpha}.
\item{} \code{crossing.df}: Data frame of crossing points with indices, years, statistics, and AMS.
\item{} \code{change.df}: Subset of \code{crossing.df} with statistically significant crossings.
\item{} \code{p.value}: Two-sided p-value assessing the significance of maximum crossing statistic.
\item{} \code{reject}: Logical indicating whether null hypothesis of no change point is rejected.
\item{} \code{msg}: Character string summarizing the test result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{References}
Sneyers, R. (1990). On the statistical analysis of series of observations.
Technical note No. 143, World Meteorological Organization, Geneva.
\end{References}
%
\begin{SeeAlso}
\LinkA{mks.plot}{mks.plot}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
mks.test(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{mle.estimation}{Maximum Likelihood Parameter Estimation}{mle.estimation}
%
\begin{Description}
Estimates parameters of a probability distribution (\code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO},
\code{GNO}, \code{PE3}, \code{LP3}, or \code{WEI}) or non-stationary variant by maximizing the log‐likelihood.
Initial values are obtained through L‐moment parameter estimation, and optimization is
performed via \LinkA{nlminb}{nlminb} with repeated perturbations if needed.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mle.estimation(data, years, model, prior = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{model}] Character (1); string specifying the probability model. The first three
letters denote the family: \code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3},
\code{LP3}, or \code{WEI}. A trailing signature of \code{10} or \code{100} indicates a linear trend
in location; \code{11} or \code{110} indicates linear trends in both location and scale.

\item[\code{prior}] Numeric (2); optional vector of parameters \eqn{(p, q)}{} that specifies
the parameters of a Beta prior on \eqn{\kappa}{}. Only works with models \code{GEV},
\code{GEV100}, and \code{GEV110}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
\begin{enumerate}

\item{} Calls \LinkA{pelxxx}{pelxxx} on \code{data} to obtain initial parameter estimates.
\item{} Initializes trend parameters to zero if there is a trailing signature.
\item{} For \code{WEI} models, sets the location parameter to zero to ensure support.
\item{} Defines an objective function as the negative of the \LinkA{llvxxx}{llvxxx} function.
\item{} Runs \LinkA{nlminb}{nlminb} with box constraints. Attempts optimization
up to 100 times.

\end{enumerate}

\end{Details}
%
\begin{Value}
List; results of parameter estimation:
\begin{itemize}

\item{} \code{params}: Numeric vector of estimated parameters.
\item{} \code{mll}: Maximum log‐likelihood value.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{llvxxx}{llvxxx}, \LinkA{gllxxx}{gllxxx}, \LinkA{pelxxx}{pelxxx}, \LinkA{nlminb}{nlminb}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
mle.estimation(data, years, "GNO100")

\end{ExampleCode}
\end{Examples}
\HeaderA{model.assessment}{Evaluate Goodness-of-Fit for Fitted Flood Models}{model.assessment}
%
\begin{Description}
Computes multiple performance metrics and diagnostic indicators to assess the quality of
a fitted flood frequency model This includes residual statistics, information criteria,
and coverage-based metrics using bootstrapped confidence intervals.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
model.assessment(
  data,
  years,
  model,
  params,
  uncertainty,
  pp.formula = "Weibull",
  alpha = 0.05
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{model}] Character; string specifying the probability model. The first three
letters denote the family: \code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3},
\code{LP3}, or \code{WEI}. A trailing signature of \code{10} or \code{100} indicates a linear trend
in location; \code{11} or \code{110} indicates linear trends in both location and scale.

\item[\code{params}] Numeric; a vector of fitted model parameters generated by
\LinkA{pelxxx}{pelxxx} or \LinkA{mle.estimation}{mle.estimation}.

\item[\code{uncertainty}] List; quantiles and confidence intervals
generated by \LinkA{sb.uncertainty}{sb.uncertainty} or \LinkA{rfpl.uncertainty}{rfpl.uncertainty}.

\item[\code{pp.formula}] Character (1); string specifying the plotting position formula.
One of \code{"Weibull"}, \code{"Blom"}, \code{"Cunnane"}, \code{"Gringorten"}, or \code{"Hazen"}
(default is "Weibull").

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
List; model assessment metrics:
\begin{itemize}

\item{} \code{estimates}: Quantile estimates for empirical return periods.
\item{} \code{R2}: Coefficient of determination from linear regression of estimates vs. data.
\item{} \code{RMSE}: Root mean squared error of quantile estimates.
\item{} \code{Bias}: Mean bias of quantile estimates.
\item{} \code{AIC}: Akaike Information Criterion.
\item{} \code{BIC}: Bayesian Information Criterion.
\item{} \code{AIC\_MLL}: Akaike Information Criterion, computed using the maximum log-likelihood.
\item{} \code{BIC\_MLL}: Bayesian Information Criterion, computed using the maximum log-likelihood.
\item{} \code{AW}: Average width of the confidence interval(s).
\item{} \code{POC}: Percent of observations covered by the confidence interval(s).
\item{} \code{CWI}: Confidence width index, a metric that combines \code{AW} and \code{POC}.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{sb.uncertainty}{sb.uncertainty}, \LinkA{rfpl.uncertainty}{rfpl.uncertainty},
\LinkA{lm}{lm}, \LinkA{approx}{approx}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize example data, years, and params
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
params <- c(100, 10)

# Perform uncertainty analysis
uncertainty <- sb.uncertainty(data, years, "NOR", "L-moments")

# Perform model assessment
model.assessment(data, years, "NOR", params, uncertainty)

\end{ExampleCode}
\end{Examples}
\HeaderA{mw.variance}{Estimate Variance for Annual Maximum Streamflow Data}{mw.variance}
%
\begin{Description}
This function estimates the standard deviation of a vector of annual maximum
streamflow (AMS) data using a moving window algorithm, returning a list that
pairs each window’s mean year with its computed standard deviation. The
parameters \code{size} and \code{step} parameters control the behaviour of the window.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mw.variance(data, years, size = 10, step = 5)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{size}] Integer (1); the number of consecutive indices in each moving
window. Must be a positive integer less than or equal to \code{length(data)}
(default is 10). If \code{length(data) < size}, an error is raised.

\item[\code{step}] Integer (1); the offset (in indices) between successive moving
windows. Must be a positive integer (default is 5).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with two entries:
\begin{itemize}

\item{} \code{years}: Numeric; the mean year within each window.
\item{} \code{std}: Numeric; the standard deviation of the data within each window.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
mw.variance(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{pel-functions}{Parameter Estimation with L-Moments}{pel.Rdash.functions}
\aliasA{pelgev}{pel-functions}{pelgev}
\aliasA{pelglo}{pel-functions}{pelglo}
\aliasA{pelgno}{pel-functions}{pelgno}
\aliasA{pelgum}{pel-functions}{pelgum}
\aliasA{pellno}{pel-functions}{pellno}
\aliasA{pellp3}{pel-functions}{pellp3}
\aliasA{pelnor}{pel-functions}{pelnor}
\aliasA{pelpe3}{pel-functions}{pelpe3}
\aliasA{pelwei}{pel-functions}{pelwei}
%
\begin{Description}
Estimate the parameters of one of nine different distributions (\code{GUM}, \code{NOR},
\code{LNO}, \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3}, \code{LP3}, and \code{WEI}) using the method of L-moments.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pelgum(data)

pelnor(data)

pellno(data)

pelgev(data)

pelglo(data)

pelgno(data)

pelpe3(data)

pellp3(data)

pelwei(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
First, the sample L-moments of the data are computed using the \LinkA{lmom.sample}{lmom.sample}
method. Then formulas from Hosking (1997) are used to compute the parameters from the
L-moments. Distributions \code{GNO}, \code{PE3}, and \code{LP3} use a rational approximation to compute
the parameters.
\end{Details}
%
\begin{Value}
Numeric; a vector of parameters.
\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
pellp3(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{pelkap}{L-Moments Parameter Estimation for the Kappa Distribution}{pelkap}
%
\begin{Description}
This functions estimates the parameters of the four-parameter Kappa distribution using
the method of L-moments. Since there is no known closed form solution for the parameters
in terms of the L-moments, the parameters are computed numerically using Newton-Raphson
iteration.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pelkap(data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
First, the sample L-moments of the data are computed using the \LinkA{lmom.sample}{lmom.sample}
method. Then, the \LinkA{optim}{optim} function is used to determine the
parameters by minimizing the \code{sumquad.tau3tau4} helper function. The implementation of
this routine is based on the deprecated \code{homtest} package.
\end{Details}
%
\begin{Value}
Numeric (4); a vector of parameters.
\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
pelkap(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{pelxxx}{Helper Function for L-moments Parameter Estimation}{pelxxx}
%
\begin{Description}
A helper function used by \LinkA{pel-functions}{pel.Rdash.functions}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pelxxx(model, data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] Character; \code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO},
\code{PE3}, \code{LP3}, or \code{WEI}.

\item[\code{data}] Numeric; a vector of annual maximum streamflow data.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Numeric; a vector of parameters.
\begin{itemize}

\item{} Numeric (2) if \code{model} is \code{GUM}, \code{NOR}, or \code{LNO}.
\item{} Numeric (3) if \code{model} is \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3}, \code{LP3}, or \code{WEI}.

\end{itemize}

\end{Value}
%
\begin{References}
Hosking, J.R.M. \& Wallis, J.R., 1997. Regional frequency analysis: an approach based
on L-Moments. Cambridge University Press, New York, USA.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
pelxxx("PE3", data)

\end{ExampleCode}
\end{Examples}
\HeaderA{pettitt.plot}{Plot Results from the Pettitt Change‐Point Test}{pettitt.plot}
%
\begin{Description}
Creates a two‐panel visualization of the Mann–Whitney–Pettitt test. The
upper panel plots the Pettitt \eqn{U_t}{} statistic over time along with the
significance threshold and potential change point. The lower panel displays
the annual maximum streamflow (AMS) data with an optional trend line,
estimates of the mean, and potential change point.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pettitt.plot(data, years, results, show_trend = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{results}] List; results of the MKS test, generated by \LinkA{pettitt.test}{pettitt.test}.

\item[\code{show\_trend}] Logical; if TRUE (default), draw a fitted line through the AMS data.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{patchwork}; A plot object with two \code{ggplot2} panels stacked vertically.
\end{Value}
%
\begin{SeeAlso}
\LinkA{pettitt.test}{pettitt.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
results <- pettitt.test(data, years)
pettitt.plot(data, years, results)

\end{ExampleCode}
\end{Examples}
\HeaderA{pettitt.test}{Pettitt Test for Abrupt Changes in the Mean of a Time Series}{pettitt.test}
%
\begin{Description}
Performs the non-parametric Pettitt test to detect a single change point in the
mean of a time series, often used for abrupt shifts in hydrological data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pettitt.test(data, years, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The Pettitt test is a rank-based non-parametric test that evaluates the
hypothesis of a change point in the median/mean of a time series.
It computes the maximum of the absolute value of the U-statistic over all
possible split points. The p-value is approximated using an asymptotic formula.
\end{Details}
%
\begin{Value}
A named list containing:
\begin{itemize}

\item{} \code{ut}: Vector of absolute U-statistics for all time indices.
\item{} \code{k.statistic}: Maximum absolute U-statistic (test statistic).
\item{} \code{k.critical}: Critical K-statistic value for given \code{alpha}.
\item{} \code{p.value}: Approximate p-value for the test.
\item{} \code{change.index}: Index of the detected change point (0 if none).
\item{} \code{change.year}: Year of the detected change point (0 if none).
\item{} \code{reject}: Logical. TRUE if the null hypothesis was rejected.
\item{} \code{msg}: Formatted summary message describing the test result.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{pettitt.plot}{pettitt.plot}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
pettitt.test(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{pp.test}{Phillips–Perron Unit Root Test}{pp.test}
%
\begin{Description}
Applies the Phillips–Perron (PP) test to check for a unit root in annual
maximum streamflow (AMS) data. The null hypothesis is that the series contains a
unit root (and is thus non-stationary). This implementation of the PP test assumes
the  time series has both stationary drift and a linear trend.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pp.test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The implementation of this test is based on the \pkg{aTSA} package, which interpolates
p-values from a table of critical values presented in Fuller W. A. (1996). The
critical values are only available for \eqn{\alpha \geq 0.01}{}. Therefore, a reported
p-value of 0.01 indicates \eqn{p \leq 0.01}{}.
\end{Details}
%
\begin{Value}
List; the test results, consisting of:
\begin{itemize}

\item{} \code{statistic}: The Z-statistic used to perform the test.
\item{} \code{p.value}: Reported p-value from the test. See notes on interpolation thresholds.
\item{} \code{reject}: Logical. TRUE if the null hypothesis of a unit root is rejected at \code{alpha}.
\item{} \code{msg}: Character string summarizing the test result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{References}
Fuller, W. A. (1996). Introduction to statistical time series, second ed., Wiley,
New York.
\end{References}
%
\begin{SeeAlso}
\LinkA{kpss.test}{kpss.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
pp.test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{qnt-functions}{Quantile Functions for Probability Models}{qnt.Rdash.functions}
\aliasA{qntgev}{qnt-functions}{qntgev}
\aliasA{qntgev100}{qnt-functions}{qntgev100}
\aliasA{qntgev110}{qnt-functions}{qntgev110}
\aliasA{qntglo}{qnt-functions}{qntglo}
\aliasA{qntglo100}{qnt-functions}{qntglo100}
\aliasA{qntglo110}{qnt-functions}{qntglo110}
\aliasA{qntgno}{qnt-functions}{qntgno}
\aliasA{qntgno100}{qnt-functions}{qntgno100}
\aliasA{qntgno110}{qnt-functions}{qntgno110}
\aliasA{qntgum}{qnt-functions}{qntgum}
\aliasA{qntgum10}{qnt-functions}{qntgum10}
\aliasA{qntgum11}{qnt-functions}{qntgum11}
\aliasA{qntkap}{qnt-functions}{qntkap}
\aliasA{qntlno}{qnt-functions}{qntlno}
\aliasA{qntlno10}{qnt-functions}{qntlno10}
\aliasA{qntlno11}{qnt-functions}{qntlno11}
\aliasA{qntlp3}{qnt-functions}{qntlp3}
\aliasA{qntlp3100}{qnt-functions}{qntlp3100}
\aliasA{qntlp3110}{qnt-functions}{qntlp3110}
\aliasA{qntnor}{qnt-functions}{qntnor}
\aliasA{qntnor10}{qnt-functions}{qntnor10}
\aliasA{qntnor11}{qnt-functions}{qntnor11}
\aliasA{qntpe3}{qnt-functions}{qntpe3}
\aliasA{qntpe3100}{qnt-functions}{qntpe3100}
\aliasA{qntpe3110}{qnt-functions}{qntpe3110}
\aliasA{qntwei}{qnt-functions}{qntwei}
\aliasA{qntwei100}{qnt-functions}{qntwei100}
\aliasA{qntwei110}{qnt-functions}{qntwei110}
%
\begin{Description}
Compute the quantiles for stationary and non-stationary variants
of nine different distributions (\code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO},
\code{GNO}, \code{PE3}, \code{LP3}, and \code{WEI}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
qntgum(p, params, years = NULL)

qntgum10(p, params, years)

qntgum11(p, params, years)

qntnor(p, params, years = NULL)

qntnor10(p, params, years)

qntnor11(p, params, years)

qntlno(p, params, years = NULL)

qntlno10(p, params, years)

qntlno11(p, params, years)

qntgev(p, params, years = NULL)

qntgev100(p, params, years)

qntgev110(p, params, years)

qntglo(p, params, years = NULL)

qntglo100(p, params, years)

qntglo110(p, params, years)

qntgno(p, params, years = NULL)

qntgno100(p, params, years)

qntgno110(p, params, years)

qntpe3(p, params, years = NULL)

qntpe3100(p, params, years)

qntpe3110(p, params, years)

qntlp3(p, params, years = NULL)

qntlp3100(p, params, years)

qntlp3110(p, params, years)

qntwei(p, params, years = NULL)

qntwei100(p, params, years)

qntwei110(p, params, years)

qntkap(p, params, years = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{p}] Numeric; a vector of probabilities between 0 and 1.

\item[\code{params}] Numeric; a vector of parameters. Must have the correct length for the model.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.
Required for non-stationary models, which end in \code{10}, \code{11}, \code{100}, or \code{110}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The quantile function is the inverse of the cumulative distribution function.
For the two-parameter distributions (\code{GUM}, \code{NOR}, \code{LNO}), there are three
different \code{qnt} functions:
\begin{itemize}

\item{} \code{qnt...()}: Stationary location and scale, 2 parameters.
\item{} \code{qnt...10()}: Time-varying location, stationary scale, 3 parameters.
\item{} \code{qnt...11()}: Time-varying location and scale, 4 parameters.

\end{itemize}


For three-parameter distributions (\code{GEV}, \code{GLO}, \code{GNO}, \code{PE3}, \code{LP3}, \code{WEI}),
there are also three \code{qnt} functions:
\begin{itemize}

\item{} \code{qnt...()}: Stationary location and scale, 3 parameters.
\item{} \code{qnt...100()}: Time-varying location, stationary scale, 4 parameters.
\item{} \code{qnt...110()}: Time-varying location and scale, 5 parameters.

\end{itemize}

\end{Details}
%
\begin{Value}
If \code{p} or \code{years} is a scalar, returns a numeric vector. Otherwise, returns a matrix.
\end{Value}
%
\begin{Note}
The \code{qnt...}, functions perform extensive parameter validation, which can be slow.
If you plan to make many calls to these methods, it is recommended to use
the \LinkA{qntxxx}{qntxxx} helper function instead.
\end{Note}
%
\begin{SeeAlso}
\LinkA{qntxxx}{qntxxx}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize p, years, and params
p <- runif(n = 10)
years <- seq(from = 1901, to = 2000)
params <- c(0, 1, 1, 1, 0)

# Compute the quantiles
qntwei110(p, params, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{qntxxx}{Helper Function for Quantile Functions}{qntxxx}
%
\begin{Description}
A helper function used by \LinkA{qnt-functions}{qnt.Rdash.functions}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
qntxxx(name, signature, p, params, covariate = 0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{name}] Character (1); the name of the probability distribution.

\item[\code{signature}] Character (1); the non-stationary signature (\code{NULL}, \code{"10"}, or \code{"11"}).

\item[\code{p}] Numeric; a vector of probabilities. Must be between 0 and 1.

\item[\code{params}] Numeric; a vector of parameters. Must have the correct length for the model.

\item[\code{covariate}] Numeric; a vector with the same length as \code{data}.
Required if \code{signature} is \code{"10"} or \code{"11"}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
If \code{p} or \code{years} is a scalar, returns a numeric vector. Otherwise, returns a matrix.
\end{Value}
%
\begin{SeeAlso}
\LinkA{qnt-functions}{qnt.Rdash.functions}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize p and params
p <- runif(n = 10)
params <- c(0, 1, 0)

# Compute the log-likelihood
qntxxx("GEV", NULL, p, params)

\end{ExampleCode}
\end{Examples}
\HeaderA{rfpl.uncertainty}{Regula-Falsi Confidence Intervals for Flood Quantile Estimates}{rfpl.uncertainty}
%
\begin{Description}
Calculates estimates and confidence intervals for return levels at standard
return periods (2, 5, 10, 20, 50, and 100 years) using the profile likelihood
and Regula-Falsi root‐finding method.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
rfpl.uncertainty(
  data,
  years,
  model,
  slices = "last",
  alpha = 0.05,
  eps = 0.01,
  prior = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{model}] Character (1); string specifying the probability model. The first three
letters denote the family: \code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3},
\code{LP3}, or \code{WEI}. A trailing signature of \code{10} or \code{100} indicates a linear trend
in location; \code{11} or \code{110} indicates linear trends in both location and scale.

\item[\code{slices}] Character (1) or Numeric; specifies the years at which
to compute the estimates and confidence intervals (default is "last").
\begin{itemize}

\item{} \code{"all"}: returns estimates for all values in \code{years}.
\item{} \code{"first"}: returns estimates for first year in the dataset.
\item{} \code{"last"}: returns estimates for last year in the dataset.
\item{} Passing a numeric vector to \code{slices} allows for custom values.

\end{itemize}


If the chosen model is stationary, the results will be the same for all slices.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{eps}] Numeric (1); tolerance for the Regula-Falsi convergence (default is 0.01).

\item[\code{prior}] Numeric (2); optional vector of parameters \eqn{(p, q)}{} that specifies
the parameters of a Beta prior on \eqn{\kappa}{}. Only works with models \code{GEV},
\code{GEV100}, and \code{GEV110}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
\begin{enumerate}

\item{} Fits the model using \LinkA{mle.estimation}{mle.estimation} to obtain parameter estimates and log‐likelihood.
\item{} Defines an objective function \eqn{f(y_p, p)}{} based on the chi-squared distribution.
\item{} Iteratively brackets the root by scaling initial guesses by 0.05 until f changes sign.
\item{} Uses the Regula Falsi method to solve \eqn{f(y_p, p) = 0}{} for each return-period probability.
\item{} Returns lower and upper confidence bounds at level alpha and the quantile estimates.

\end{enumerate}

\end{Details}
%
\begin{Value}
List; quantiles and confidence intervals. Each year maps to a sub-list:
\begin{itemize}

\item{} \code{estimates}: Estimated quantiles for each return period.
\item{} \code{ci\_lower}: Lower bound of the confidence interval for each return period.
\item{} \code{ci\_upper}: Upper bound of the confidence interval for each return period.
\item{} \code{t}: Vector of return periods; \code{c(2, 5, 10, 20, 50, 100)}.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{qntxxx}{qntxxx}, \LinkA{sb.uncertainty}{sb.uncertainty}, \LinkA{nlminb}{nlminb}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
rfpl.uncertainty(data, years, "GLO110")

\end{ExampleCode}
\end{Examples}
\HeaderA{runs.plot}{Plot Runs Test Results}{runs.plot}
%
\begin{Description}
Generates a residual plot of Sen’s estimator applied to annual maximum streamflow
(AMS) data with a horizontal dashed line at zero and an annotation of the Runs
test p-value.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
runs.plot(years, residuals, results, name, show_trend = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{residuals}] Numeric; A vector of residuals produced by \LinkA{sens.trend}{sens.trend}.

\item[\code{results}] List; runs test results generated by \LinkA{runs.test}{runs.test}.

\item[\code{name}] Character; either \code{"sens-variance"} or \code{"sens-mean"}.

\item[\code{show\_trend}] Logical; if TRUE (default), draw a fitted line through the AMS data.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot containing:
\begin{itemize}

\item{} Black points for the residual at each year.
\item{} A red dashed horizontal line at \eqn{y = 0}{}.
\item{} A text annotation “Runs p-value: X.XXX” in the plot area.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{runs.test}{runs.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data and years
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)

# Generate runs test plot 
residuals <- sens.trend(data, years)$residuals
results <- runs.test(residuals)
runs.plot(years, residuals, results, "sens-mean")

\end{ExampleCode}
\end{Examples}
\HeaderA{runs.test}{Wald–Wolfowitz Runs Test for Randomness}{runs.test}
%
\begin{Description}
Applies the Wald–Wolfowitz runs test to a numeric vector of residuals in
order to assess whether they behave as a random sequence. The test statistic’s
p-value is compared to the significance level \code{alpha}, and a decision is
returned along with a human-readable summary message.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
runs.test(residuals, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{residuals}] Numeric; A vector of residuals produced by \LinkA{sens.trend}{sens.trend}.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The Wald–Wolfowitz runs test examines the sequence of residuals to test for
randomness around the median. A small p-value suggests non-random clustering,
which may indicate that a linear model is inappropriate for the data.
\end{Details}
%
\begin{Value}
List; test results, including:
\begin{itemize}

\item{} \code{p.value}: P-value from the Wald–Wolfowitz runs test applied to residuals.
\item{} \code{residuals}: Numeric vector of residual values from a fitted linear model.
\item{} \code{reject}: Logical. TRUE if the null hypothesis of random residuals is rejected.
\item{} \code{msg}: Character string summarizing the test result.

\end{itemize}

\end{Value}
%
\begin{References}
Wald, A. and Wolfowitz, J. (1940). On a test whether two samples are from the
same population. Annals of Mathematical Statistics, 11(2), 147–162.
\end{References}
%
\begin{SeeAlso}
\LinkA{runs.plot}{runs.plot}, \LinkA{sens.trend}{sens.trend}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Initialize data and years
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)

# Perform the runs test
residuals <- sens.trend(data, years)$residuals
runs.test(residuals)

\end{ExampleCode}
\end{Examples}
\HeaderA{sb.uncertainty}{Sample Bootstrap Confidence Intervals for Flood Quantile Estimates}{sb.uncertainty}
%
\begin{Description}
Computes estimates and confidence intervals for return levels at standard return periods
(2, 5, 10, 20, 50, and 100 years) using the sample bootstrap method. This function
supports a variety of probability models and parameter estimation methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
sb.uncertainty(
  data,
  years,
  model,
  method,
  slices = "last",
  n_sim = 10000,
  alpha = 0.05,
  prior = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{model}] Character (1); string specifying the probability model. The first three
letters denote the family: \code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3},
\code{LP3}, or \code{WEI}. A trailing signature of \code{10} or \code{100} indicates a linear trend
in location; \code{11} or \code{110} indicates linear trends in both location and scale.

\item[\code{method}] Character (1); string specifying the estimation method.
Must be \code{"L-moments"}, \code{"MLE"}, or \code{"GMLE"}.

\item[\code{slices}] Character (1) or Numeric; specifies the years at which
to compute the estimates and confidence intervals (default is "last").
\begin{itemize}

\item{} \code{"all"}: returns estimates for all values in \code{years}.
\item{} \code{"first"}: returns estimates for first year in the dataset.
\item{} \code{"last"}: returns estimates for last year in the dataset.
\item{} Passing a numeric vector to \code{slices} allows for custom values.

\end{itemize}


If the chosen model is stationary, the results will be the same for all slices.

\item[\code{n\_sim}] Integer (1); the number of bootstrap simulations (default is 10000).

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{prior}] Numeric (2); optional vector of parameters \eqn{(p, q)}{} that specifies
the parameters of a Beta prior on \eqn{\kappa}{}. Only works with models \code{GEV},
\code{GEV100}, and \code{GEV110}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The bootstrap procedure samples from the fitted distribution via inverse
transform sampling. For each bootstrapped sample, the parameters are re-estimated
using the specified \code{method}. Then, the bootstrapped parameters are used to compute
a new set of bootstrapped quantiles. Confidence intervals are obtained from the
empirical non-exceedance probabilities of the bootstrapped quantiles.
\end{Details}
%
\begin{Value}
List; quantiles and confidence intervals. Each year maps to a sub-list:
\begin{itemize}

\item{} \code{estimates}: Estimated quantiles for each return period.
\item{} \code{ci\_lower}: Lower bound of the confidence interval for each return period.
\item{} \code{ci\_upper}: Upper bound of the confidence interval for each return period.
\item{} \code{t}: Vector of return periods; \code{c(2, 5, 10, 20, 50, 100)}.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{pelxxx}{pelxxx}, \LinkA{mle.estimation}{mle.estimation}, \LinkA{lmom.sample}{lmom.sample}, \LinkA{quantile}{quantile}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
sb.uncertainty(data, years, "WEI", "L-moments")

\end{ExampleCode}
\end{Examples}
\HeaderA{sens.plot}{Plot Sen’s Trend Estimator}{sens.plot}
%
\begin{Description}
Produces a scatter plot of the annual maximum streamflow (AMS) data or its variance
against time, overlaid with Sen’s trend estimator and an annotation of the fitted
equation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
sens.plot(data, years, results, name, show_trend = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{results}] List; runs test results generated by \LinkA{sens.trend}{sens.trend}.

\item[\code{name}] Character; either \code{"sens-variance"} or \code{"sens-mean"}.

\item[\code{show\_trend}] Logical; if TRUE (default), draw a fitted line through the AMS data.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot containing:
\begin{itemize}

\item{} Black points for each year’s AMS (or variance) value.
\item{} Optional black line connecting the AMS data if \code{show\_trend = TRUE}.
\item{} Blue line representing Sen’s trend estimator.
\item{} A text annotation displaying the fitted equation \eqn{y = mx + b}{}.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{sens.trend}{sens.trend}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
results <- sens.trend(data, years)
sens.plot(data, years, results, "sens-mean")

\end{ExampleCode}
\end{Examples}
\HeaderA{sens.trend}{Sen's Trend Estimator}{sens.trend}
%
\begin{Description}
Computes Sen's trend estimator for a univariate time series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
sens.trend(data, years, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Sen's slope estimator is a robust, non-parametric trend estimator computed from the median
of all pairwise slopes between data points. The corresponding intercept is taken as the
median of residual-corrected values.
\end{Details}
%
\begin{Value}
List; the estimated trend:
\begin{itemize}

\item{} \code{sens.slope}: Median slope of all pairwise data-year combinations.
\item{} \code{sens.intercept}: Median intercept estimate of the fitted line.
\item{} \code{residuals}: Vector of residuals between observed and fitted values.
\item{} \code{msg}: Character string summarizing the results.

\end{itemize}

\end{Value}
%
\begin{References}
Sen, P.K. (1968). Estimates of the regression coefficient based on Kendall's tau.
\emph{Journal of the American Statistical Association}, 63(324), 1379–1389.
\end{References}
%
\begin{SeeAlso}
\LinkA{runs.test}{runs.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
sens.trend(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{spearman.plot}{Plot Spearman’s Rho Autocorrelation}{spearman.plot}
%
\begin{Description}
Visualizes Spearman’s rho autocorrelation coefficients with
shaded points indicating statistical significance.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
spearman.plot(results)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{results}] List; autocorrelation coefficients generated by \LinkA{spearman.test}{spearman.test}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot showing:
\begin{itemize}

\item{} Vertical segments from \eqn{y=0}{} up to each \eqn{\rho}{} value at its lag.
\item{} Filled circles at each lag, filled black if serial correlation is detected.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
results <- spearman.test(data)
spearman.plot(results)

\end{ExampleCode}
\end{Examples}
\HeaderA{spearman.test}{Spearman Test for Autocorrelation}{spearman.test}
%
\begin{Description}
Performs the Spearman rank autocorrelation test on annual maximum streamflow (AMS) data to
check for autocorrelation at various lags. Reports the first lag where the autocorrelation
is no longer statistically significant at the given significance level.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
spearman.test(data, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
For each lag from \eqn{1}{} to \eqn{n - 3}{}, the function computes the Spearman correlation
coefficient between the AMS series and its lagged version. The first lag with an
insignificant autocorrelation coefficient returned as \code{least.lag}.
\end{Details}
%
\begin{Value}
List; test results, including:
\begin{itemize}

\item{} \code{rho}: Vector of Spearman autocorrelation estimates for lags \eqn{1}{} to \eqn{n - 3}{}.
\item{} \code{sig}: Logical vector indicating which lags exhibit significant autocorrelation.
\item{} \code{least.lag}: The smallest lag at which the autocorrelation is not statistically significant.
\item{} \code{reject}: Logical. TRUE if \code{least.lag > 0}.
\item{} \code{msg}: Character string summarizing the test result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{cor.test}{cor.test}, \LinkA{bbmk.test}{bbmk.test}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
spearman.test(data)

\end{ExampleCode}
\end{Examples}
\HeaderA{uncertainty.plot}{Plot Return Levels and Confidence Intervals}{uncertainty.plot}
%
\begin{Description}
Constructs a time–series plot of estimated return levels along with their
associated confidence intervals. The confidence bounds are shown as a shaded ribbon,
and the point estimates are overlaid as a solid line. The return periods are
displayed on a logarithmic scale.

If the specified model is non-stationary, the \code{slice} argument is required
and the return levels are reported as "Effective Return Periods".
\end{Description}
%
\begin{Usage}
\begin{verbatim}
uncertainty.plot(model, results, slice = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] Character (1); string specifying the probability model. The first three
letters denote the family: \code{GUM}, \code{NOR}, \code{LNO}, \code{GEV}, \code{GLO}, \code{GNO}, \code{PE3},
\code{LP3}, or \code{WEI}. A trailing signature of \code{10} or \code{100} indicates a linear trend
in location; \code{11} or \code{110} indicates linear trends in both location and scale.

\item[\code{results}] List; generated by \LinkA{sb.uncertainty}{sb.uncertainty} or \LinkA{rfpl.uncertainty}{rfpl.uncertainty}.

\item[\code{slice}] Character (1); the anchor year for non-stationary return level estimates.
Required for models ending in \code{10}, \code{100}, \code{11}, or \code{110}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{ggplot}; a plot showing:
\begin{itemize}

\item{} A dashed black line for both the lower and upper confidence bounds.
\item{} A semi-transparent gray ribbon between \code{results\$ci\_lower} and \code{results\$ci\_upper}.
\item{} A solid blue line for the point estimates.

\end{itemize}

\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
results <- sb.uncertainty(data, years, "WEI", "L-moments")
uncertainty.plot("WEI", results)

\end{ExampleCode}
\end{Examples}
\HeaderA{white.test}{White Test for Heteroskedasticity}{white.test}
%
\begin{Description}
Performs the White test for heteroskedasticity by regressing the squared residuals of a
linear model on the original regressors and their squared terms. The null hypothesis
is homoskedasticity.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
white.test(data, years, alpha = 0.05, quiet = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{years}] Numeric; a vector of years with the same length as \code{data}.

\item[\code{alpha}] Numeric (1); the significance level (default is 0.05).

\item[\code{quiet}] Logical (1); if FALSE, prints a summary of results (default is TRUE).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The White test regresses the squared residuals from a primary linear model
\code{lm(data \textasciitilde{} years)} against both the original regressor and its square.
The test statistic is calculated as \eqn{nR^2}{}, where \eqn{R^2}{} is the
coefficient of determination from the auxiliary regression. Under the null hypothesis,
this statistic follows a \eqn{\chi^2}{} distribution with 2 degrees of freedom.
Rejection of the null hypothesis suggests heteroskedasticity in the residuals.
\end{Details}
%
\begin{Value}
List; results of the White test:
\begin{itemize}

\item{} \code{r.squared}: Coefficient of determination from the auxiliary regression.
\item{} \code{statistic}: White test statistic based on sample size and \code{r.squared}.
\item{} \code{p.value}: P-value derived from a Chi-squared distribution with 2 degrees of freedom.
\item{} \code{reject}: Logical. TRUE if the null hypothesis is rejected at significance \code{alpha}.
\item{} \code{msg}: Character string summarizing the test result (printed if \code{quiet = FALSE}).

\end{itemize}

\end{Value}
%
\begin{References}
White, H. (1980). A heteroskedasticity-consistent covariance matrix
estimator and a direct test for heteroskedasticity. \emph{Econometrica}, 48(4), 817–838.
\end{References}
%
\begin{SeeAlso}
\LinkA{lm}{lm}, \LinkA{pchisq}{pchisq}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
years <- seq(from = 1901, to = 2000)
white.test(data, years)

\end{ExampleCode}
\end{Examples}
\HeaderA{z.selection}{Z-Statistic Method for Distribution Selection}{z.selection}
%
\begin{Description}
Selects the best-fit distribution by computing a bias-corrected Z-statistic for the sample
L-kurtosis (\eqn{\tau_4}{}) against the theoretical L-moments for a set of candidate
distributions. The distribution with the smallest absolute Z-score is selected.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
z.selection(data, n_sim = 20000)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] Numeric; a vector of annual maximum streamflow data.

\item[\code{n\_sim}] Integer (1); the number of bootstrap simulations (default is 20000).
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The method performs model selection using both raw and log-transformed data. The
distributions which use the raw AMS data are GEV, GLO, PE3, GNO, and WEI. The LP3
distribution uses log-transformed data.

The Z-statistic is determined by fitting a four-parameter Kappa distribution to the
raw and log-transformed data. Then, bootstrapped samples from this Kappa distribution
The L-moments of these bootstrapped samples are used to estimate the Z-statistic
for each distribution.
\end{Details}
%
\begin{Value}
List; results of distribution selection:
\begin{itemize}

\item{} \code{method}: \code{"Z-selection"}
\item{} \code{params}: Kappa distribution parameters for the raw AMS data.
\item{} \code{log\_params}: Kappa distribution parameters for the log-transformed AMS data.
\item{} \code{bootstrap}: Bias and standard deviation of the estimated L-kurtosis.
\item{} \code{distance}: List of computed Z-statistics for each candidate distribution.
\item{} \code{recommendation}: Name of the distribution with the smallest Z-statistic.

\end{itemize}

\end{Value}
%
\begin{SeeAlso}
\LinkA{ld.selection}{ld.selection}, \LinkA{lk.selection}{lk.selection}, \LinkA{pelkap}{pelkap}, \LinkA{qntxxx}{qntxxx}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data <- rnorm(n = 100, mean = 100, sd = 10)
z.selection(data)

\end{ExampleCode}
\end{Examples}
\printindex{}
\end{document}
